\section{Conclusion and Limitation}
\label{sec:conclusion&limitation}

We present Point Ladder Tuning (PLT), a parameter-efficient fine-tuning method for point cloud analysis. PLT keeps the backbone network frozen while using a hierarchical Ladder Network (HLN) to extract local features directly from point clouds. These local features are combined with the backbone's global features through our Local-Global Fusion (LGF) module. We further enhance performance by using multi-scale features to generate adaptive prompts that optimize the backbone for specific tasks. Experiments show PLT achieves strong results in classification and segmentation with minimal added parameters. Future work could extend PLT to other tasks like object detection and generation.

%In this paper, we introduced Point Ladder Tuning (PLT), a parameter-efficient fine-tuning strategy designed specifically for point cloud analysis. PLT freezes the parameters of the backbone network and employs a hierarchical Ladder Network (HLN) to extract local information directly from the input point cloud. To effectively integrate this local information with intermediate global features from the backbone network, we proposed a Local-Global Fusion (LGF) module. Furthermore, multi-scale fused features are leveraged to generate adaptive prompts, optimizing the backbone network's global features for downstream tasks. Our experiments demonstrate that PLT achieves competitive performance in tasks such as object classification and dense prediction, while requiring minimal additional parameters. However, this study is limited to classification and segmentation tasks, leaving evaluations on other tasks, such as point cloud object detection and generation, as directions for future work.
