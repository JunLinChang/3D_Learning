\input{tab/sota}

\section{Experiments}
\label{sec:experiments}
%In this section, we conduct an extensive evaluation of the proposed Point Ladder Tuning (PLT) method across multiple point cloud datasets and tasks \lr{add citation for datasets and tasks here}. Our experiments aim to validate the effectiveness of PLT in achieving high performance while maintaining parameter efficiency, comparing it against state-of-the-art methods to highlight its advantages.

%In this section, we present a comprehensive evaluation of the proposed Point Ladder Tuning (PLT) method across diverse point cloud datasets and tasks \lr{cite datasets and tasks here}, and performed our comparison across various downstream tasks as 3D Object Classification, Few-Shot Learning, 3D Dense Prediction Task. The experimental analysis is designed to assess the efficacy of PLT in attaining competitive performance while preserving parameter efficiency.
%Furthermore, we compare PLT against state-of-the-art approaches to demonstrate its superior performance and computational advantages. An ablation study is also included.

In this section, we conduct a comprehensive evaluation of our proposed Point Ladder Tuning (PLT) method across multiple point cloud datasets \lr{cite datasets and tasks here} and diverse tasks, including 3D object classification, few-shot learning, cross-domain methods and 3D dense prediction. Our experiments systematically assess PLT's ability to achieve state-of-the-art performance while maintaining parameter efficiency. Through extensive comparisons with existing approaches, we demonstrate PLT's superior performance and computational advantages. Additionally, we present an ablation study to validate the contribution of each key component in our framework.

%\paragraph{Experimental Settings}
To ensure a fair comparison with prior fine-tuning methods~\cite{zha2023instance, zhou2024dynamic}, we maintain identical experimental setups. All experiments were conducted on an NVIDIA GeForce RTX 3090 GPU (24GB) under controlled hardware conditions. During training, we freeze the weights of the pre-trained backbone and fine-tune only a small subset of parameters in the newly introduced modules.

To rigorously evaluate our proposed method, we employ three state-of-the-art pre-trained models as baselines: Point-BERT~\cite{yu2022point}, Point-MAE~\cite{pang2022masked}, and ACT~\cite{dong2022autoencoders}. These models represent diverse pre-training paradigms, ensuring a comprehensive comparison. Detailed discussions of their performance and comparative analysis will be presented in the experiment section.

%To evaluate the effectiveness of our proposed method, we adopt three representative pre-trained models—Point-BERT~\cite{yu2022point}, Point-MAE~\cite{pang2022masked}, and ACT~\cite{dong2022autoencoders}—as baselines. 
%These models cover a diverse set of 3D pre-training paradigms and serve as strong foundations for comparison. 
%Point-BERT~\cite{yu2022point} extends the BERT~\cite{devlin2018bert} framework to 3D point clouds by leveraging a masked point modeling strategy. 
%It learns both low-level geometric structures and high-level semantic features by aligning masked point embeddings with discrete tokens generated by a dVAE-based tokenizer~\cite{vahdat2018dvae}. 
%Point-MAE~\cite{pang2022masked} introduces the masked auto-encoder paradigm into point cloud pre-training, where the model reconstructs masked point patches from the visible ones. 
%This encourages the network to capture meaningful latent representations that generalize well to downstream tasks. 
%To mitigate the challenge of limited labeled 3D data, ACT~\cite{dong2022autoencoders} utilizes a cross-modal teacher-student framework. 
%It transfers knowledge from a fine-tuned cross-modal pre-trained teacher to a 3D point cloud learner, enabling the latter to acquire rich semantic representations through distillation.


\lr{delete or move to corresponding section}
To comprehensively evaluate our proposed method, we select three state-of-the-art pre-trained models as baselines: Point-BERT~\cite{yu2022point}, Point-MAE~\cite{pang2022masked}, and ACT~\cite{dong2022autoencoders}. These models represent diverse 3D pre-training paradigms, providing robust reference points for comparative analysis.
Point-BERT~\cite{yu2022point} adapts the BERT framework~\cite{devlin2018bert} to point cloud processing through masked point modeling. The approach employs a dVAE-based tokenizer~\cite{vahdat2018dvae} to generate discrete tokens, enabling simultaneous learning of geometric structures and semantic features through masked embedding alignment.
Point-MAE~\cite{pang2022masked} implements a masked autoencoder architecture for point cloud pre-training. By reconstructing occluded point patches from visible regions, the model learns transferable latent representations that demonstrate strong generalization across downstream tasks.
To mitigate the challenge of limited labeled 3D data, ACT~\cite{dong2022autoencoders} utilizes a cross-modal teacher-student framework. It transfers knowledge from a fine-tuned cross-modal pre-trained teacher to a 3D point cloud learner, enabling the latter to acquire rich semantic representations through distillation.
%ACT~\cite{dong2022autoencoders} addresses the limited availability of labeled 3D data through a teacher-student learning approach. The method works by first training a "teacher" model on cross-modal knowledge, then transferring this knowledge to a "student" model that works specifically with 3D point clouds. This knowledge transfer helps the 3D model learn better features even with limited point cloud training data.


\subsection{3D Object Classification}
\label{sec:classification}

\textbf{Object Classification on Synthetic Dataset.} We conduct experiments on the ModelNet dataset~\cite{wu20153d}, which contains 12,311 clean 3D CAD models spanning 40 categories. Following the protocol in DAPT~\cite{zhou2024dynamic}, we split ModelNet40 into 9,843 training samples and 2,468 testing samples. During training, we applied standard data augmentation techniques, including random scaling and random translation. The training configuration adheres to the settings used for baseline comparisons. Specifically, the AdamW optimizer is utilized with an initial learning rate of $5 \times 10^{-4}$ and a weight decay of 0.05. A cosine learning rate scheduler with a 10-epoch warm-up period is employed. The model is trained for a total of 300 epochs with a batch size of 32. 

As shown in Tab.~\ref{tab:sota}, without voting, PLT achieves accuracy rates of 93.8\%, 93.5\% and 93.6\% in Point-MAE, Point-BERT and ACT respectively, representing gains of \textbf{0.6\%}, \textbf{0.8\%} and \textbf{0.3 \%} over full fine-tuning. When using voting, PLT continues to outperform full fine-tuning, notably improving Point-BERT’s accuracy by an additional 1.0\%. Compare to other prior arts, our method achieve best performance by fine-tuning Point-BERT, due to \lr{the reason why Point-BERT is best but others is not.}. \lr{add the reason of performance gain here, discuss why there is performance gain.}

\textbf{Object Classification on Real-World Dataset.} While most pre-trained point cloud models are trained on synthetic datasets like ShapeNet~\cite{chang2015shapenet}, which consist of clean, uniformly distributed point clouds, real-world point clouds often present additional challenges such as noise, missing data, and diverse distributions. To evaluate performance in these more realistic scenarios, we use the ScanObjectNN dataset~\cite{uy2019revisiting}, which comprises approximately 15k point cloud samples across 15 categories captured in indoor scenes, often containing background interference and occlusions. The training configuration is consistent with those used for ModelNet~\cite{wu20153d}. As shown in Tab.~\ref{tab:sota}, we evaluate PLT on three variants of the ScanObjectNN dataset (OBJ\_BG, OBJ\_ONLY, and PB\_T50\_RS), using Point-BERT~\cite{yu2022point},  Point-MAE~\cite{pang2022masked} and ACT~\cite{dong2022autoencoders} as baselines.

Our PLT method demonstrates notable accuracy improvements over full fine-tuning in all settings, using only 2.71\% of the parameters. 
Specifically, PLT achieves gains of 4.14\%, 1.73\%, and 3.02\% on the three ScanObjectNN variants with Point-BERT. 
Compared to the state-of-the-art model, PointGST~\cite{liang2024parameter}, PLT achieves accuracy improvements of 0.45\% on Point-BERT~\cite{yu2022point} and 0.17\% on Point-MAE~\cite{pang2022masked} under the most challenging PB\_T50\_RS setting in ScanObjectNN~\cite{uy2019revisiting}, showcasing the robustness and efficiency of our approach in handling real-world noisy and occluded point cloud data. 
Furthermore, PLT exhibits the least performance degradation on the ACT~\cite{dong2022autoencoders} baseline among all point cloud PEFT approaches. 
While other methods suffer from significant accuracy drops, our approach maintains strong performance, underscoring its architectural strength in preserving semantic representations during adaptation. 
Together, these results validate PLT’s superiority in both performance and parameter efficiency in the real scene dataset.

\input{tab/fewshot}

\subsection{Few-Shot Learning}
\label{sec:few_shot}

We further evaluate the transferability of PLT in a few-shot learning setting using the ModelNet40~\cite{wu20153d} dataset, which serves as a standard benchmark for assessing the efficiency of data usage in low-resource settings. The training configuration follows that of the 3D object classification task, with the exception that the number of training epochs is set to 150. Following prior works~\cite{zha2023instance, zhou2024dynamic}, we adopt the standard n-way m-shot protocol, where $n \in \{5, 10\}$ and $m \in \{10, 20\}$.

As summarized in Tab.~\ref{tab:fewshot}, our PLT consistently outperforms both full fine-tuning and state-of-the-art PEFT methods across the majority of settings, regardless of the choice of pre-trained backbone (Point-BERT~\cite{yu2022point}, Point-MAE~\cite{pang2022masked}, or ACT~\cite{dong2022autoencoders}). For example, under the 5-way 20-shot setting, our PLT achieves a overall accuracy of 98.8\% with Point-BERT~\cite{yu2022point}, outperforming other methods such as full fine-tuning (+2.5\%) and PointGST~\cite{liang2024parameter} (+0.9\%). These results highlight our PLT's strong capability to generalize under data-scarce conditions and validate its effectiveness in few-shot point cloud learning tasks. \lr{add performance gain reason, e.g., hierarchy structure, local-global feature fusion, token, etc}

In addition to superior accuracy, our PLT also demonstrates improved stability across few-shot settings. As shown in Tab.~\ref{tab:fewshot}, our method yields consistently lower or comparable standard deviations compared to other PEFT baselines. For instance, under the 5-way 20-shot setting, PLT achieves a standard deviation of only $\pm$1.1\% with Point-BERT, outperforming other methods such as LST~\cite{sung2022lst} ($\pm$1.8\%) and PointGST~\cite{liang2024parameter} ($\pm$2.0\%). This trend is observed across multiple configurations, indicating that PLT not only improves mean performance but also reduces performance variance across different folds. Such stability is particularly critical in few-shot learning scenarios, where training data is limited and model robustness becomes more important.

\subsection{Comparison with Cross-domain PEFT Methods}
\label{sec:compare}

We compare our PLT with a broad range of PEFT methods originally proposed for NLP and 2D vision, adapting them to point cloud scenarios. As shown in Tab.~\ref{tab:origin_finetuning}, methods like VPT~\cite{jia2022visual} and Adapter~\cite{houlsby2019parameter} suffer significant accuracy drops when transferred to the 3D domain. For example, VPT~\cite{jia2022visual} leads to a 4.09\% decrease on the challenging PB\_T50\_RS variant compared to full fine-tuning. Similarly, although Adapter~\cite{houlsby2019parameter} achieves moderate gains on OBJ\_ONLY, it performs poorly on more complex tasks.

LST~\cite{sung2022lst} provides competitive performance in specific settings, but its generalization across variants is limited. In contrast, PLT achieves consistently strong results across various challenging scenarios, while tuning only 0.6M parameters, a fraction of the full model size.

Further comparisons in Tab.~\ref{tab:compare} highlight PLT’s superiority under parameter-efficient fine-tuning (PEFT) settings. It outperforms all other PEFT methods, including the strongest 3D-specific methods like PointGST~\cite{liang2024parameter} and DAPT~\cite{zhou2024dynamic}, especially on the most difficult variant PB\_T50\_RS.

These results reveal that many PEFT approaches from other domains struggle with 3D data due to domain gaps and architectural mismatches. Even recent 3D-specific methods face challenges in balancing accuracy and parameter efficiency. In contrast, PLT offers robust generalization, lightweight adaptation, and state-of-the-art performance, establishing itself as a unified and effective PEFT solution for 3D point cloud classification. \lr{Performance not that good due to domain gaps, why the PLT has better performance than others, due to skip connection?}

\input{tab/compare}

\input{tab/origin_finetuning}

\subsection{3D Dense Prediction Task}
For dense prediction tasks, including part segmentation and semantic segmentation, we adopt a prediction head similar to that of PointNext~\cite{qian2022pointnext}, allowing us to effectively utilize multi-resolution information and enhance performance while maintaining a low number of trainable parameters.

We validate the effectiveness of PLT on the ShapeNetPart dataset~\cite{yi2016scalable}, comprising 16,881 samples distributed across 16 object categories and 50 part categories. Training is performed using the AdamW optimizer with a weight decay of 0.05 and a cosine learning rate scheduler. The initial learning rate is set to $2 \times 10^{-4}$, with a warm-up period of 10 epochs. The models are trained for 300 epochs using a batch size of 16. As shown in Tab.~\ref{tab:segmentation}, PLT achieves results comparable to other methods in terms of instance-level mIoU (Inst. mIoU) while achieving notable improvements in class-level mIoU (Cls. mIoU). Specifically, PLT improves Inst. mIoU on PointBERT~\cite{yu2022point} by 0.5\% over DAPT~\cite{zhou2024dynamic}, demonstrating its effectiveness in capturing fine-grained details.

\input{tab/segmentation}

To comprehensively evaluate the effectiveness of our proposed PLT framework on dense prediction tasks, we conduct semantic segmentation experiments on two widely adopted large-scale indoor scene datasets: S3DIS~\cite{armeni20163d}and ScanNetV2~\cite{dai2017scannet}, with results summarized in Tab.~\ref{tab:semantic_segmentation}. 

S3DIS~\cite{armeni20163d} is a large-scale indoor scene dataset comprising six areas with a total of 273 million points annotated across 13 categories. Following established practices~\cite{dong2022autoencoders}, Area 5 is recommended for evaluation to provide a more reliable and standardized assessment of performance in semantic segmentation tasks. The model is trained using a cosine learning rate scheduler with an initial learning rate of $2 \times 10^{-4}$. Training is performed over 60 epochs with a batch size of 32, while other configurations are consistent with those used for ShapeNetPart~\cite{yi2016scalable}.

ScanNetV2~\cite{dai2017scannet}is a large-scale benchmark dataset for indoor 3D scene understanding, encompassing a diverse range of environments, from compact residential and office spaces to expansive public and commercial buildings. The dataset comprises 1,513 RGB-D scanned scenes with annotations for 20 semantic categories. Following standard practice, 1,201 scenes are used for training and 312 scenes for testing.For training, we adopt a cosine annealing learning rate scheduler, with an initial learning rate set to $5 \times 10^{-3}$. The models are trained for 500 epochs using a batch size of 32.

As reported in Tab.~\ref{tab:semantic_segmentation}, PLT consistently achieves superior performance across all baselines and datasets, while maintaining a significantly lower number of tunable parameters (2.04M) compared to full fine-tuning (27.02M for Point-MAE~\cite{pang2022masked}, for example). On S3DIS~\cite{armeni20163d}, PLT outperforms all PEFT competitors under each backbone. When applied to ACT~\cite{dong2022autoencoders} as the backbone, PLT achieves an mAcc of 70.6\% and mIoU of 61.5\%, improving over the closest competitor PPT~\cite{zhang2024positional} by 2.3\% in mIoU. Similar trends are observed for Point-MAE~\cite{pang2022masked} and Point-BERT~\cite{yu2022point}, with PLT consistently pushing mIoU to above 61\%, outperforming alternatives such as IDPT~\cite{zha2023instance}, DAPT~\cite{zhou2024dynamic}, and PointGST~\cite{liang2024parameter}. \lr{add more discussion about why there is performance gain..., e.g.,  a better hierarchical sampling rate for point cloud, local-global fusion scheme etc, compare to PointGST or others.}

On the more complex ScanNetV2 dataset~\cite{dai2017scannet}, which demands strong generalization due to its large-scale and real-world variability, PLT again demonstrates robust performance. With the Point-BERT~\cite{yu2022point} backbone, PLT reaches a voxel mIoU (VmIoU) of 48.2\% and point mIoU (PmIoU) of 47.8\%, leading the field in both metrics. Notably, the performance gap between PLT and existing methods becomes more evident here: PLT surpasses DAPT~\cite{zhou2024dynamic} and PointGST~\cite{liang2024parameter} by 4.6\% and 1.9\% respectively on VmIoU, further underscoring its strong adaptability in real-world 3D segmentation scenarios. \lr{performance gain comes from a better sampling accuracy, therefore has more accuracy segmentation than other methods, etc. revise this paragraph, highlight the reason of performance gain and novelty.}

These results reinforce the effectiveness of PLT as a lightweight and generalizable PEFT framework for 3D semantic segmentation. Compared with recent 3D-specific PEFT methods (e.g., PointGST~\cite{liang2024parameter}, DAPT~\cite{zhou2024dynamic}), PLT delivers consistent improvements across multiple backbones and datasets, achieving higher accuracy with fewer trainable parameters. Its strong performance on both S3DIS~\cite{armeni20163d} and ScanNetV2~\cite{dai2017scannet} highlights PLT’s ability to retain dense spatial and global contextual information, making it particularly well-suited for challenging 3D scene dense prediction tasks.

\input{tab/semantic}

\input{tab/part}

\input{tab/ablation}

\input{tab/fusion_way}

\input{tab/sidenet}

\subsection{Ablation Study}

\textbf{Effectiveness of PLT Components}. We first investigate the contribution of each individual module in PLT through controlled ablation studies. As shown in Table VII, removing any of the three core components—Scale \& Shift Fine-tuning (SSF), Dynamic Prompting (DP), or the Hierarchical Ladder Network (HLN)—leads to a notable drop in accuracy on the most challenging ScanObjectNN variant (PB\_T50\_RS). As shown in Tab.~\ref{tab:part}, excluding HLN results in a performance decline from 85.53\% to 83.34\%, while excluding SSF or DP alone reduces accuracy to 84.52\% and 84.66\%, respectively. These results clearly demonstrate the complementary roles of all three modules, highlighting that the full integration of SSF, DP, and HLN is essential for maximizing transferability and performance efficiency.

\textbf{Hyperparameter Ablation for Hierarchical Ladder Network.} We conduct a detailed ablation study on the hyperparameters of the HLN, including the number of nearest neighbors $K$, feature dimensions $d$, and the number of hierarchical layers. As summarized in Tab.\ref{tab:ablation}, performance peaks when using $K=[16,8,4]$, feature dimensions $d=[16,32,64,128]$, and a 3-layer architecture, achieving an accuracy of 85.53\% with only 0.60M tunable parameters. This design follows an intuitive hierarchical principle: as the depth of the network increases, the point cloud is progressively downsampled, leading to a reduced number of points in deeper layers. Correspondingly, the number of nearest neighbors $K$ is decreased layer by layer to ensure that neighborhood aggregation remains local and meaningful. If a large $K$ were retained in deeper layers, the receptive field would become overly large relative to the reduced spatial resolution, potentially diluting fine-grained local cues and introducing noise from unrelated distant points. By contrast, using a decreasing neighbor size, such as $K=[16,8,4]$, strikes a balance between capturing sufficient local context in shallow layers and preserving locality in deeper layers. This adaptive design improves the model’s ability to learn multi-scale features across hierarchical levels, leading to superior performance in downstream tasks.

\textbf{Comparison Between HLN and PLT.} To further quantify the value of HLN and its integration within PLT, we compare their standalone performances in Tab.\ref{tab:sidenet}. While HLN alone achieves 80.74\%, integrating it into the full PLT framework improves accuracy to 85.53\%, outperforming even full fine-tuning (85.18\%) with less than 3\% of the tunable parameters. This underscores the importance of coupling lightweight local reasoning structures like HLN with pretrained global backbones for optimal performance.

\textbf{Fusion Strategy for Local and Global Information.} The integration strategy of local and global information plays a critical role in downstream performance. In Tab.~\ref{tab:fusion_way}, we evaluate several fusion mechanisms. The proposed Local-Global Fusion (LGF) with Softmax achieves the highest accuracy (85.53\%), outperforming additive, concatenative, and single-source approaches. Notably, leveraging only local features yields a considerably lower accuracy (82.86\%) than relying solely on global features (84.52\%), reflecting the benefits of pretraining on global representation learning.

\textbf{Attention Score of LGF}. To gain deeper insight into the behavior of LGF, we visualize the local attention weights $s_l$ across categories and network layers in Fig.~\ref{fig:attention_score}. The results reveal three key patterns:

\begin{enumerate}
    \item The local attention scores are generally lower than 0.5, reaffirming the reliance on pretrained global priors.
    \item Category-specific variation is more pronounced in early layers and becomes increasingly uniform in deeper layers, suggesting a hierarchical fusion where shallow layers capture category-aware local details while deeper layers consolidate generalized global semantics.
    \item A downward trend in attention to local features from the first to third layers further supports the notion that the network progressively shifts focus from fine-grained local patterns to more abstract global concepts.
\end{enumerate}

These findings collectively validate the design of PLT as a lightweight yet powerful PEFT framework for 3D transfer learning, wherein carefully balanced architectural and fusion strategies contribute to superior accuracy and efficiency.

\input{fig/supplement/attention_score}

\input{tab/performance}

\subsection{Visualization}

\textbf{The Visualization of t-SNE Analysis}. Fig.~\ref{fig:tsne} presents the t-SNE~\cite{van2008visualizing} visualizations of feature manifolds generated by fully fine-tuned methods, point-cloud-specific fine-tuning approaches such as IDPT~\cite{zha2023instance} and DAPT~\cite{zhou2024dynamic}, and our proposed PLT, all trained on the ScanObjectNN PB\_T50\_RS dataset~\cite{uy2019revisiting}. The dispersion of points between categories in the t-SNE plot reflects the quality of the model's feature representation—greater inter-cluster dispersion and smaller intra-cluster average distance indicate stronger discriminative power, facilitating easier classification.

Notably, our PLT demonstrates significantly greater inter-cluster dispersion and a smaller intra-cluster average distance compared to previous methods. This suggests that our approach effectively enhances feature separation between categories while maintaining compactness within each category, indicative of a superior ability to learn and represent discriminative features.

Furthermore, PLT achieves these results while using fewer learnable parameters, showcasing its efficiency in leveraging pre-trained models for adaptation to downstream tasks. This not only reduces computational cost but also highlights PLT's potential for broader applicability in resource-constrained environments. In comparison to fully fine-tuned and point-cloud-specific methods, PLT strikes an optimal balance between parameter efficiency and representation quality, underscoring its value in point cloud learning tasks.

\textbf{Semantic Segmentation Visualizations}. Fig.~\ref{fig:s3dis_1} presents a qualitative comparison of segmentation results between our proposed PLT method and prior fine-tuning approaches in semantic segmentation tasks on Area 5 of the S3DIS dataset~\cite{armeni20163d}. The comparison highlights the advantages of PLT in addressing common challenges in point cloud segmentation. From the first row of the figure, it is evident that while full fine-tuning is resource-intensive, it struggles to accurately segment object boundaries. This often results in poor boundary delineation and misclassification in continuous regions, primarily due to insufficient capture of local point cloud information. 

A more detailed comparison in the office\_9 scene reveals that methods such as full fine-tuning, IDPT~\cite{zha2023instance}, and DAPT~\cite{zhou2024dynamic} incorrectly classify walls as beams and cluttered regions as ceilings. In contrast, our PLT method successfully segments clutter regions while significantly reducing the misclassification of walls. Similarly, in the office\_35 scene, PLT outperforms other methods in segmenting challenging areas such as the cluttered region and the board. Whereas methods like full fine-tuning, IDPT~\cite{zha2023instance}, and PPT~\cite{zhang2024positional} fail to accurately recognize or segment these regions, PLT demonstrates superior performance, maintaining clarity and precision.
\lr{highlight the specific case, e.g., hallway\_10 shows a more accurate segmentation results, due to a hierarchical sampling and local-global fusion scheme. The proposed approach demonstrates robustness to local feature variations while maintaining precise boundary delineation capabilities. Thus, shows our advance in method, etc. Discuss the reason for our methods.} 

These results emphasize the ability of the PLT method to deliver outstanding segmentation performance while using far fewer fine-tuning parameters. Compared to other approaches, PLT not only provides clearer segmentation boundaries but also significantly reduces misclassifications, further validating its efficiency and effectiveness for point cloud semantic segmentation tasks.

\input{fig/supplement/semantic_segmentation}
\input{fig/tsne}

\subsection{Performance Analysis}

Tab.\ref{tab:performance} provides a comprehensive comparison of our PLT framework against various recent parameter-efficient tuning methods built upon the Point-MAE\cite{pang2022masked} baseline. The evaluation is conducted on the most challenging variant of ScanObjectNN~\cite{uy2019revisiting} (i.e., PB\_T50\_RS), with consistent settings including a batch size of 32 and all throughput metrics measured on a single NVIDIA RTX 3090 GPU.

Our method achieves the best accuracy (85.53\%) among all competing approaches, outperforming the original full fine-tuning baseline (85.18\%) and other prompt tuning or adapter-based methods. Notably, PLT accomplishes this with only 0.60M trainable parameters—merely 2.7\% of the full Point-MAE model—demonstrating remarkable parameter efficiency. In comparison, methods such as IDPT~\cite{zha2023instance} (1.70M) and DAPT~\cite{zhou2024dynamic} (1.09M) use significantly more parameters while yielding inferior accuracy.

In terms of computational complexity, PLT maintains a moderate FLOPs level (5.02GFLOPs), comparable to the baseline PointMAE~\cite{pang2022masked} (4.93GFLOPs) and much lower than IDPT~\cite{zha2023instance} (7.27GFLOPs), further validating its lightweight design. From a practical deployment standpoint, PLT achieves favorable training throughput (165.4 samples/s) and inference speed (210.8 samples/s), striking a solid balance between speed and effectiveness. While DAPT offers slightly higher training throughput (169.0 samples/s), its inference accuracy and parameter efficiency are not as competitive.

Memory usage is also a critical factor. PLT maintains low inference memory (0.85G), on par with most methods and substantially better than PointPEFT~\cite{tang2024point} (1.12G). Although the incorporation of FPS and KNN operations in PLT leads to a moderate increase in training memory consumption (2.26G), the overhead remains acceptable in practical applications. Notably, PLT still maintains a significantly lower memory footprint compared to other methods (e.g. PointPEFT~\cite{tang2024point} (8.64G) and IDPT~\cite{zha2023instance} (5.10G)) that also rely on similar operations.

Overall, PLT demonstrates strong performance across all dimensions—achieving top-tier accuracy with minimal parameters, competitive FLOPs, efficient inference and training speeds, and modest memory consumption. This highlights its suitability for real-world deployment where both effectiveness and efficiency are essential. Importantly, PLT achieves this without sacrificing the backbone architecture or requiring extensive architectural re-design, offering a plug-and-play solution compatible with powerful pretrained models like Point-MAE~\cite{pang2022masked}.