\input{tab/sota}

\section{Experiments}
\label{sec:experiments}
%In this section, we conduct an extensive evaluation of the proposed Point Ladder Tuning (PLT) method across multiple point cloud datasets and tasks \lr{add citation for datasets and tasks here}. Our experiments aim to validate the effectiveness of PLT in achieving high performance while maintaining parameter efficiency, comparing it against state-of-the-art methods to highlight its advantages.

%In this section, we present a comprehensive evaluation of the proposed Point Ladder Tuning (PLT) method across diverse point cloud datasets and tasks \lr{cite datasets and tasks here}, and performed our comparison across various downstream tasks as 3D Object Classification, Few-Shot Learning, 3D Dense Prediction Task. The experimental analysis is designed to assess the efficacy of PLT in attaining competitive performance while preserving parameter efficiency.
%Furthermore, we compare PLT against state-of-the-art approaches to demonstrate its superior performance and computational advantages. An ablation study is also included.

In this section, we conduct a comprehensive evaluation of our proposed Point Ladder Tuning (PLT) method across multiple point cloud datasets~\cite{wu20153d, uy2019revisiting, yi2016scalable, armeni20163d, dai2017scannet} and diverse tasks, including 3D object classification, few-shot learning, cross-domain methods and 3D dense prediction. Our experiments systematically assess PLT's ability to achieve state-of-the-art performance while maintaining parameter efficiency. Through extensive comparisons with existing approaches, we demonstrate PLT's superior performance and computational advantages. Additionally, we present an ablation study to validate the contribution of each key component in our framework.

%\paragraph{Experimental Settings}
To ensure a fair comparison with prior fine-tuning methods~\cite{zha2023instance, zhou2024dynamic}, we maintain identical experimental setups. All experiments were conducted on an NVIDIA GeForce RTX 3090 GPU (24GB) under controlled hardware conditions. During training, we freeze the weights of the pre-trained backbone and fine-tune only a small subset of parameters in the newly introduced modules.

To ensure a comprehensive comparison, we employ three state-of-the-art pre-trained models as baselines: Point-BERT~\cite{yu2022point}, Point-MAE~\cite{pang2022masked}, and ACT~\cite{dong2022autoencoders}. These models represent diverse pre-training paradigms, shown with detailed discussions in the experiment section.

%To evaluate the effectiveness of our proposed method, we adopt three representative pre-trained models—Point-BERT~\cite{yu2022point}, Point-MAE~\cite{pang2022masked}, and ACT~\cite{dong2022autoencoders}—as baselines. 
%These models cover a diverse set of 3D pre-training paradigms and serve as strong foundations for comparison. 
%Point-BERT~\cite{yu2022point} extends the BERT~\cite{devlin2018bert} framework to 3D point clouds by leveraging a masked point modeling strategy. 
%It learns both low-level geometric structures and high-level semantic features by aligning masked point embeddings with discrete tokens generated by a dVAE-based tokenizer~\cite{vahdat2018dvae}. 
%Point-MAE~\cite{pang2022masked} introduces the masked auto-encoder paradigm into point cloud pre-training, where the model reconstructs masked point patches from the visible ones. 
%This encourages the network to capture meaningful latent representations that generalize well to downstream tasks. 
%To mitigate the challenge of limited labeled 3D data, ACT~\cite{dong2022autoencoders} utilizes a cross-modal teacher-student framework. 
%It transfers knowledge from a fine-tuned cross-modal pre-trained teacher to a 3D point cloud learner, enabling the latter to acquire rich semantic representations through distillation.


% \lr{delete or move to corresponding section}
% To comprehensively evaluate our proposed method, we select three state-of-the-art pre-trained models as baselines: Point-BERT~\cite{yu2022point}, Point-MAE~\cite{pang2022masked}, and ACT~\cite{dong2022autoencoders}. These models represent diverse 3D pre-training paradigms, providing robust reference points for comparative analysis.
% Point-BERT~\cite{yu2022point} adapts the BERT framework~\cite{devlin2018bert} to point cloud processing through masked point modeling. The approach employs a dVAE-based tokenizer~\cite{vahdat2018dvae} to generate discrete tokens, enabling simultaneous learning of geometric structures and semantic features through masked embedding alignment.
% Point-MAE~\cite{pang2022masked} implements a masked autoencoder architecture for point cloud pre-training. By reconstructing occluded point patches from visible regions, the model learns transferable latent representations that demonstrate strong generalization across downstream tasks.
% To mitigate the challenge of limited labeled 3D data, ACT~\cite{dong2022autoencoders} utilizes a cross-modal teacher-student framework. It transfers knowledge from a fine-tuned cross-modal pre-trained teacher to a 3D point cloud learner, enabling the latter to acquire rich semantic representations through distillation.
%ACT~\cite{dong2022autoencoders} addresses the limited availability of labeled 3D data through a teacher-student learning approach. The method works by first training a "teacher" model on cross-modal knowledge, then transferring this knowledge to a "student" model that works specifically with 3D point clouds. This knowledge transfer helps the 3D model learn better features even with limited point cloud training data.


\subsection{3D Object Classification}
\label{sec:classification}

The analysis of object classification results on synthetic dataset is detailed in the supplementary materials

% \textbf{Object Classification on Synthetic Dataset.} We conduct experiments on the ModelNet dataset~\cite{wu20153d}, which contains 12,311 clean 3D CAD models spanning 40 categories. Following the protocol in DAPT~\cite{zhou2024dynamic}, we split ModelNet40 into 9,843 training samples and 2,468 testing samples. During training, we applied standard data augmentation techniques, including random scaling and random translation. The training configuration adheres to the settings used for baseline comparisons. Specifically, the AdamW optimizer is utilized with an initial learning rate of $5 \times 10^{-4}$ and a weight decay of 0.05. A cosine learning rate scheduler with a 10-epoch warm-up period is employed. The model is trained for a total of 300 epochs with a batch size of 32. 

% As shown in Tab.~\ref{tab:sota}, without voting, PLT achieves accuracy rates of 93.8\%, 93.5\% and 93.6\% in Point-MAE, Point-BERT and ACT respectively, representing gains of \textbf{0.6\%}, \textbf{0.8\%} and \textbf{0.6\%} over full fine-tuning. When using voting, PLT continues to outperform full fine-tuning, notably improving Point-BERT's accuracy by an additional \textbf{1.0\%}. These results demonstrate the effectiveness of PLT for point cloud classification, as it explicitly models local structures on the original point cloud while jointly leveraging global and local representations to enhance performance.

\textbf{Object Classification on Real-World Dataset.} While most pre-trained point cloud models are trained on synthetic datasets like ShapeNet~\cite{chang2015shapenet}, which consist of clean, uniformly distributed point clouds, real-world point clouds often present additional challenges such as noise, missing data, and diverse distributions. To evaluate performance in these more realistic scenarios, we use the ScanObjectNN dataset~\cite{uy2019revisiting}, which comprises approximately 15k point cloud samples across 15 categories captured in indoor scenes, often containing background interference and occlusions. The training configuration is consistent with those used for ModelNet~\cite{wu20153d}. As shown in Tab.~\ref{tab:sota}, we evaluate PLT on three variants of the ScanObjectNN dataset (OBJ\_BG, OBJ\_ONLY, and PB\_T50\_RS), using Point-BERT~\cite{yu2022point},  Point-MAE~\cite{pang2022masked} and ACT~\cite{dong2022autoencoders} as baselines.

Our PLT method demonstrates notable accuracy improvements over full fine-tuning in all settings, using only 2.71\% of the parameters. 
Specifically, PLT achieves gains of 4.14\%, 1.73\%, and 3.02\% on the three ScanObjectNN variants with Point-BERT. 
Compared to the state-of-the-art model, PointGST~\cite{liang2024parameter}, PLT achieves accuracy improvements of \textbf{0.45\%} on Point-BERT~\cite{yu2022point} and \textbf{0.24\%} on Point-MAE~\cite{pang2022masked} under the most challenging PB\_T50\_RS setting in ScanObjectNN~\cite{uy2019revisiting}, showcasing the robustness and efficiency of our approach in handling real-world noisy and occluded point cloud data. 
Furthermore, PLT exhibits the least performance degradation on the ACT~\cite{dong2022autoencoders} baseline among all point cloud PEFT approaches. 
While other methods suffer from significant accuracy drops, our approach maintains strong performance, underscoring its architectural strength in preserving semantic representations during adaptation. 
Together, these results validate PLT’s superiority in both performance and parameter efficiency in the real scene dataset.

\input{tab/fewshot}

\subsection{Few-Shot Learning}
\label{sec:few_shot}

We further evaluate the transferability of PLT in a few-shot learning setting using the ModelNet40~\cite{wu20153d} dataset, which serves as a standard benchmark for assessing the efficiency of data usage in low-resource settings. The training configuration follows that of the 3D object classification task, with the exception that the number of training epochs is set to 150. Following prior works~\cite{zha2023instance, zhou2024dynamic}, we adopt the standard n-way m-shot protocol, where $n \in \{5, 10\}$ and $m \in \{10, 20\}$.

As summarized in Tab.~\ref{tab:fewshot}, our PLT consistently outperforms both full fine-tuning and state-of-the-art PEFT methods across the majority of settings, regardless of the choice of pre-trained backbone (Point-BERT~\cite{yu2022point}, Point-MAE~\cite{pang2022masked}, or ACT~\cite{dong2022autoencoders}). For example, under the 5-way 20-shot setting, our PLT achieves a overall accuracy of 98.8\% with Point-BERT~\cite{yu2022point}, outperforming other methods such as full fine-tuning (\textbf{+2.5\%}) and PointGST~\cite{liang2024parameter} (\textbf{+0.9\%}). These results highlight our PLT's strong capability to generalize under data-scarce conditions and validate its effectiveness in few-shot point cloud learning tasks. This superior performance is primarily attributed to PLT’s ability to explicitly extract hierarchical local features from the original point cloud, thereby introducing a strong inductive bias that enhances generalization during fine-tuning.

In addition to superior accuracy, our PLT also demonstrates improved stability across few-shot settings. As shown in Tab.~\ref{tab:fewshot}, our method yields consistently lower or comparable standard deviations compared to other PEFT baselines. For instance, under the 5-way 20-shot setting, PLT achieves a standard deviation of only $\pm$1.1\% with Point-BERT, outperforming other methods such as LST~\cite{sung2022lst} ($\pm$1.8\%) and PointGST~\cite{liang2024parameter} ($\pm$2.0\%). This trend is observed across multiple configurations, indicating that PLT not only improves mean performance but also reduces performance variance across different settings. Such stability is particularly critical in few-shot learning scenarios, where training data is limited and model robustness becomes more important.

\subsection{Comparison with Cross-domain PEFT Methods}
\label{sec:compare}

We compare our PLT with a broad range of PEFT methods originally proposed for NLP and 2D vision, adapting them to point cloud scenarios. As shown in Tab.~\ref{tab:origin_finetuning}, methods like VPT~\cite{jia2022visual} and Adapter~\cite{houlsby2019parameter} suffer significant accuracy drops when transferred to the 3D domain. For example, VPT~\cite{jia2022visual} leads to a 4.09\% decrease on the challenging PB\_T50\_RS variant compared to full fine-tuning. Similarly, although Adapter~\cite{houlsby2019parameter} achieves moderate gains on OBJ\_ONLY, it performs poorly on more complex tasks.

LST~\cite{sung2022lst} provides competitive performance in specific settings, but its generalization across variants is limited. In contrast, PLT achieves consistently strong results across various challenging scenarios, while tuning only 0.6M parameters, a fraction of the full model size.

Further comparisons in Tab.~\ref{tab:compare} highlight PLT's superiority under parameter-efficient fine-tuning (PEFT) settings. It outperforms all other PEFT methods, including the strongest 3D-specific methods like PointGST~\cite{liang2024parameter} and DAPT~\cite{zhou2024dynamic}, especially on the most difficult variant PB\_T50\_RS.

These findings highlight a fundamental limitation of many cross-domain PEFT methods: they often underperform in 3D settings due to domain gaps and architectural mismatches. Specifically, point cloud data is inherently unordered, sparse, and irregular in structure—characteristics not well captured by PEFT techniques originally designed for grid-like data in 2D vision or sequential data in NLP.

By contrast, our PLT is designed to explicitly address these 3D-specific challenges. It integrates hierarchical local feature extraction through HLN and incorporates both global and multi-scale local cues via adaptive fusion, enabling it to maintain high performance even in data-scarce or structurally complex scenarios. This design not only ensures strong generalization but also introduces a beneficial inductive bias tailored to 3D spatial structures. As a result, PLT emerges as a unified and effective PEFT solution for point cloud classification, striking an optimal balance between efficiency and accuracy.

\input{tab/compare}

\input{tab/origin_finetuning}

\subsection{3D Dense Prediction Task}
For dense prediction tasks, including part segmentation and semantic segmentation, we adopt a prediction head similar to that of PointNext~\cite{qian2022pointnext}, allowing us to effectively utilize multi-resolution information and enhance performance while maintaining a low number of trainable parameters.

We validate the effectiveness of PLT on the ShapeNetPart dataset~\cite{yi2016scalable}, comprising 16,881 samples distributed across 16 object categories and 50 part categories. Training is performed using the AdamW optimizer with a weight decay of 0.05 and a cosine learning rate scheduler. The initial learning rate is set to $2 \times 10^{-4}$, with a warm-up period of 10 epochs. The models are trained for 300 epochs using a batch size of 16. As shown in Tab.~\ref{tab:segmentation}, PLT achieves results comparable to other methods in terms of instance-level mIoU (Inst. mIoU) while achieving notable improvements in class-level mIoU (Cls. mIoU). Specifically, PLT improves Inst. mIoU on PointBERT~\cite{yu2022point} by \textbf{0.5\%} over DAPT~\cite{zhou2024dynamic}, demonstrating its effectiveness in capturing fine-grained details.

\input{tab/segmentation}

To comprehensively evaluate the effectiveness of our proposed PLT framework on dense prediction tasks, we conduct semantic segmentation experiments on two widely adopted large-scale indoor scene datasets: S3DIS~\cite{armeni20163d}and ScanNetV2~\cite{dai2017scannet}, with results summarized in Tab.~\ref{tab:semantic_segmentation}. 

S3DIS~\cite{armeni20163d} is a large-scale indoor scene dataset comprising six areas with a total of 273 million points annotated across 13 categories. Following established practices~\cite{dong2022autoencoders}, Area 5 is recommended for evaluation to provide a more reliable and standardized assessment of performance in semantic segmentation tasks. The model is trained using a cosine learning rate scheduler with an initial learning rate of $2 \times 10^{-4}$. Training is performed over 60 epochs with a batch size of 32, while other configurations are consistent with those used for ShapeNetPart~\cite{yi2016scalable}.

ScanNetV2~\cite{dai2017scannet}is a large-scale benchmark dataset for indoor 3D scene understanding, encompassing a diverse range of environments, from compact residential and office spaces to expansive public and commercial buildings. The dataset comprises 1,513 RGB-D scanned scenes with annotations for 20 semantic categories. Following standard practice~\cite{dai2017scannet}, 1,201 scenes are used for training and 312 scenes for testing.For training, we adopt a cosine annealing learning rate scheduler, with an initial learning rate set to $5 \times 10^{-3}$. The models are trained for 500 epochs using a batch size of 32.

As reported in Tab.~\ref{tab:semantic_segmentation}, PLT consistently achieves superior performance across all baselines and datasets, while maintaining a significantly lower number of tunable parameters (2.04M) compared to full fine-tuning (27.02M for Point-MAE~\cite{pang2022masked}, for example). On S3DIS~\cite{armeni20163d}, PLT outperforms all PEFT competitors under each backbone. When applied to ACT~\cite{dong2022autoencoders} as the backbone, PLT achieves an mAcc of 70.6\% and mIoU of 61.5\%, improving over the closest competitor PPT~\cite{zhang2024positional} by \textbf{2.3\%} in mIoU. Similar trends are observed for Point-MAE~\cite{pang2022masked} and Point-BERT~\cite{yu2022point}, with PLT consistently pushing mIoU to above 61\%, outperforming alternatives such as IDPT~\cite{zha2023instance}, DAPT~\cite{zhou2024dynamic}, and PointGST~\cite{liang2024parameter}.

On the more complex ScanNetV2 dataset~\cite{dai2017scannet}, which demands strong generalization due to its large-scale and real-world variability, PLT again demonstrates robust performance. With the Point-BERT~\cite{yu2022point} backbone, PLT reaches a voxel mIoU (VmIoU) of 48.2\% and point mIoU (PmIoU) of 47.8\%, leading the field in both metrics. Notably, the performance gap between PLT and existing methods becomes more evident here: PLT surpasses DAPT~\cite{zhou2024dynamic} and PointGST~\cite{liang2024parameter} by \textbf{4.5\%} and \textbf{2.0\%} respectively on PmIoU for the Point-BERT~\cite{yu2022point} backbone, further underscoring its strong adaptability in real-world 3D segmentation scenarios.

These results reinforce the effectiveness of PLT as a lightweight and generalizable PEFT framework for 3D semantic segmentation. Compared with recent 3D-specific PEFT methods (e.g., PointGST~\cite{liang2024parameter}, DAPT~\cite{zhou2024dynamic}), PLT delivers consistent improvements across multiple backbones and datasets, achieving higher accuracy with fewer trainable parameters. 

The key to PLT's success lies in its Hierarchical Local Network (HLN), which explicitly captures multi-resolution semantic information from the original point cloud. This design enables the model to retain fine-grained spatial details and encode rich global context during the decoding stage. By incorporating both local and global cues in a structured and resolution-aware manner, HLN provides strong inductive bias and greatly enhances the model's capability in dense 3D scene prediction. Consequently, PLT proves particularly effective in segmentation tasks that demand precise localization and semantic consistency across varying spatial scales.

\input{tab/semantic}

\input{tab/part}

\input{fig/supplement/attention_score}

\input{tab/ablation}

\input{tab/fusion_way}

\input{tab/sidenet}

\subsection{Ablation Study}

\textbf{Effectiveness of PLT Components}. We first investigate the contribution of each individual module in PLT through controlled ablation studies. As shown in Table VII, removing any of the three core components—Scale \& Shift Fine-tuning (SSF), Dynamic Prompt (DP), or the Hierarchical Ladder Network (HLN)—leads to a notable drop in accuracy on the most challenging ScanObjectNN variant (PB\_T50\_RS). As shown in Tab.~\ref{tab:part}, excluding HLN results in a performance decline from 85.53\% to 83.34\%, while excluding SSF or DP alone reduces accuracy to 84.52\% and 84.66\%, respectively. These results clearly demonstrate the complementary roles of all three modules, highlighting that the full integration of SSF, DP, and HLN is essential for maximizing transferability and performance efficiency.

\textbf{Hyperparameter Ablation for Hierarchical Ladder Network.} We conduct a detailed ablation study on the hyperparameters of the HLN, including the number of nearest neighbors $K$, feature dimensions $d$, and the number of hierarchical layers. As summarized in Tab.\ref{tab:ablation}, performance peaks when using $K=[16,8,4]$, feature dimensions $d=[16,32,64,128]$, and a 3-layer architecture, achieving an accuracy of 85.53\% with only 0.60M tunable parameters. This design follows an intuitive hierarchical principle: as the depth of the network increases, the point cloud is progressively downsampled, leading to a reduced number of points in deeper layers. Correspondingly, the number of nearest neighbors $K$ is decreased layer by layer to ensure that neighborhood aggregation remains local and meaningful. If a large $K$ were retained in deeper layers, the receptive field would become overly large relative to the reduced spatial resolution, potentially diluting fine-grained local cues and introducing noise from unrelated distant points. By contrast, using a decreasing neighbor size, such as $K=[16,8,4]$, strikes a balance between capturing sufficient local context in shallow layers and preserving locality in deeper layers. This adaptive design improves the model’s ability to learn multi-scale features across hierarchical levels, leading to superior performance in downstream tasks.

\textbf{Comparison Between HLN and PLT.} To further quantify the value of HLN and its integration within PLT, we compare their standalone performances in Tab.\ref{tab:sidenet}. While HLN alone achieves 80.74\%, integrating it into the full PLT framework improves accuracy to 85.53\%, outperforming even full fine-tuning (85.18\%) with less than 3\% of the tunable parameters. This underscores the importance of coupling lightweight local reasoning structures like HLN with pretrained global backbones for optimal performance.

\textbf{Fusion Strategy for Local and Global Information.} The integration strategy of local and global information plays a critical role in downstream performance. In Tab.~\ref{tab:fusion_way}, we evaluate several fusion mechanisms. The proposed Local-Global Fusion (LGF) with Softmax achieves the highest accuracy (85.53\%), outperforming additive, concatenative, and single-source approaches. Notably, leveraging only local features yields a considerably lower accuracy (82.86\%) than relying solely on global features (84.52\%), reflecting the benefits of pretrained global representation learning.

%\textbf{Attention Score of LGF}. To gain deeper insight into the behavior of LGF, we visualize the local attention weights $s_l$ across categories and network layers in Fig.~\ref{fig:attention_score}. The results reveal three key patterns:
%
%\begin{enumerate}
%    \item The local attention scores are generally lower than 0.5, reaffirming the reliance on pretrained global priors.
%    \item Category-specific variation is more pronounced in early layers and becomes increasingly uniform in deeper layers, suggesting a hierarchical fusion where shallow layers capture category-aware local details while deeper layers consolidate generalized global semantics.
%    \item A downward trend in attention to local features from the first to third layers further supports the notion that the network progressively shifts focus from fine-grained local patterns to more abstract global concepts.
%\end{enumerate}

\textbf{Attention Score of LGF.} Fig.~\ref{fig:attention_score} visualizes local attention weights $s_l$ across categories and layers, revealing three key insights: (1) Scores remain below 0.5, confirming dependence on pretrained global priors; (2) Early layers show strong category-specific variation that diminishes in deeper layers, indicating hierarchical fusion from local details to global semantics; (3) The decreasing attention from first to third layers demonstrates the network's progressive shift from local patterns to abstract concepts.
These findings collectively validate the design of PLT as a lightweight yet powerful PEFT framework for 3D transfer learning, wherein carefully balanced architectural and fusion strategies contribute to superior accuracy and efficiency.

\input{tab/performance}

\subsection{Visualization}

\textbf{The Visualization of t-SNE Analysis}. Fig.~\ref{fig:tsne} presents the t-SNE~\cite{van2008visualizing} visualizations of feature manifolds generated by fully fine-tuning methods, point-cloud-specific fine-tuning approaches such as IDPT~\cite{zha2023instance} and DAPT~\cite{zhou2024dynamic}, and our proposed PLT, all trained on the ScanObjectNN PB\_T50\_RS dataset~\cite{uy2019revisiting}. The dispersion of points between categories in the t-SNE plot reflects the quality of the model's feature representation, greater inter-cluster dispersion and smaller intra-cluster average distance indicate stronger discriminative power, facilitating easier classification.

Notably, our PLT demonstrates significantly greater inter-cluster dispersion and a smaller intra-cluster average distance compared to previous works. This suggests that our approach effectively enhances feature separation between categories while maintaining compactness, indicative of a superior ability to represent discriminative features.

Furthermore, PLT achieves these results while using fewer learnable parameters, showcasing its efficiency in leveraging pre-trained models for adaptation to downstream tasks. This not only reduces computational cost but also highlights PLT's potential for broader applicability in resource-constrained environments. In comparison to fully fine-tuning and point-cloud-specific PEFT methods, PLT strikes an optimal balance between parameter efficiency and representation quality, underscoring its value in point cloud learning tasks.

\textbf{Semantic Segmentation Visualizations}. Fig.~\ref{fig:s3dis_1} presents a qualitative comparison of segmentation results between our proposed PLT method and prior fine-tuning approaches in semantic segmentation tasks on Area 5 of the S3DIS dataset~\cite{armeni20163d}. The comparison highlights the advantages of PLT in addressing common challenges in point cloud segmentation. From the first row, it is evident that although full fine-tuning utilizes all model parameters, it often fails to segment object boundaries precisely, leading to blurred edges and misclassification in contiguous areas. This is mainly due to insufficient modeling of local geometric features and lack of multi-scale contextual information.

%A more detailed comparison in the office\_9 scene reveals that methods such as full fine-tuning, IDPT~\cite{zha2023instance}, and DAPT~\cite{zhou2024dynamic} incorrectly classify walls as beams and cluttered regions as ceilings. In contrast, PLT produces more accurate predictions, especially for structural boundaries, owing to its hierarchical local network (HLN), which extracts multi-resolution semantic features and preserves local structural integrity. Similarly, in the office\_35 scene, PLT outperforms other methods in segmenting challenging areas such as the cluttered region and the board. Whereas methods like full fine-tuning, IDPT~\cite{zha2023instance}, and PPT~\cite{zhang2024positional} fail to accurately recognize or segment these regions, PLT demonstrates superior performance, maintaining clarity and precision.

In office\_9, baseline methods (full fine-tuning, IDPT~\cite{zha2023instance}, DAPT~\cite{zhou2024dynamic}) misclassify walls as beams and clutter as ceilings, while PLT achieves accurate boundary segmentation through its hierarchical local network (HLN). This advantage extends to office\_35, where PLT correctly segments challenging areas (clutter, boards) that confuse other methods (PPT~\cite{zhang2024positional}, IDPT). The HLN's multi-resolution feature extraction enables precise structural understanding where conventional approaches fail.

PLT's performance gains stem from its hierarchical feature extraction and local-global fusion. By aggregating multi-resolution features while preserving local details, PLT achieves superior boundary delineation and fine-grained recognition, crucial for 3D scene understanding. Compared to alternatives, PLT delivers: (1) cleaner segmentation with fewer parameters, (2) robust performance across diverse layouts, and (3) precise semantic accuracy, establishing it as an efficient PEFT solution for point cloud segmentation.

%These improvements can be attributed to PLT's hierarchical feature extraction and local-global fusion strategy. By progressively aggregating point features at multiple resolutions and explicitly fusing global context, PLT achieves strong spatial generalization while retaining detail-rich local semantics. This enables it to better delineate object boundaries and differentiate fine-grained structures—key advantages in dense 3D scene understanding.
%In summary, PLT not only achieves visually cleaner and semantically more accurate segmentation results with substantially fewer trainable parameters, but also demonstrates robustness and precision across a wide range of spatial layouts. These qualitative results reinforce PLT's effectiveness and efficiency as a strong PEFT solution for 3D point cloud semantic segmentation.

\input{fig/s3dis}
\input{fig/tsne}

\subsection{Performance Analysis}

Tab.\ref{tab:performance} provides a comprehensive comparison of our PLT framework against various recent parameter-efficient tuning methods built upon the Point-MAE\cite{pang2022masked} baseline. The evaluation is conducted on the most challenging variant of ScanObjectNN~\cite{uy2019revisiting} (i.e., PB\_T50\_RS), with consistent settings including a batch size of 32 and all throughput metrics measured on a single NVIDIA RTX 3090 GPU.

Our method achieves the best accuracy (85.53\%) among all competing approaches, outperforming the original full fine-tuning baseline (85.18\%) and other prompt tuning or adapter-based methods. Notably, PLT accomplishes this with only 0.60M trainable parameters—merely 2.7\% of the full Point-MAE model—demonstrating remarkable parameter efficiency. In comparison, methods such as IDPT~\cite{zha2023instance} (1.70M) and DAPT~\cite{zhou2024dynamic} (1.09M) use significantly more parameters while yielding inferior accuracy.

In terms of computational complexity, PLT maintains a moderate FLOPs level (5.02GFLOPs), comparable to the baseline PointMAE~\cite{pang2022masked} (4.93GFLOPs) and much lower than IDPT~\cite{zha2023instance} (7.27GFLOPs), further validating its lightweight design. From a practical deployment standpoint, PLT achieves favorable training throughput (165.4 samples/s) and inference speed (210.8 samples/s), striking a solid balance between speed and effectiveness. While DAPT offers slightly higher training throughput (169.0 samples/s), its accuracy and parameter efficiency are not as competitive.

%Memory usage is also a critical factor. PLT maintains low inference memory (0.85G), on par with most methods and substantially better than PointPEFT~\cite{tang2024point} (1.12G). Although the incorporation of FPS and KNN operations in PLT leads to a moderate increase in training memory consumption (2.26G), the overhead remains acceptable in practical applications. Notably, PLT still maintains a significantly lower memory footprint compared to other methods (e.g. PointPEFT~\cite{tang2024point} (8.64G) and IDPT~\cite{zha2023instance} (5.10G)) that also rely on similar operations.

Memory efficiency is another key advantage of PLT. During inference, PLT maintains a low memory footprint (0.85GB), comparable to most methods and significantly better than PointPEFT~\cite{tang2024point} (1.12GB). While training memory increases moderately to 2.26GB due to FPS and KNN operations, this remains practical and is still substantially lower than alternatives like PointPEFT (8.64GB) and IDPT~\cite{zha2023instance} (5.10GB) that use similar operations.

Overall, PLT demonstrates strong performance across all dimensions—achieving top-tier accuracy with minimal parameters, competitive FLOPs, efficient inference and training speeds, and modest memory consumption. This highlights its suitability for real-world deployment where both effectiveness and efficiency are essential. Importantly, PLT achieves this without sacrificing the backbone architecture or requiring extensive architectural re-design, offering a plug-and-play solution compatible with powerful pretrained models like Point-MAE~\cite{pang2022masked}.