\section{Related Work}
\label{sec:relatedwork}

Recent advances in point cloud analysis have been driven by deep learning methods, including both supervised architectures and self-supervised pre-training techniques. Meanwhile, Parameter-Efficient Fine-Tuning (PEFT) approaches—originally developed for NLP and vision—have shown promise in adapting large pre-trained models with minimal parameter updates. In this section, we review representative methods in both areas: deep learning techniques for 3D point cloud processing and the evolution of PEFT strategies across domains.

\subsection{Deep Learning on Point Cloud}

% add more details for these methods, e.g., DGCNN, PointNet++, Point Transformer, etc.
Deep learning on point clouds has evolved significantly, with various architectures designed to capture the unique characteristics of 3D data. Early methods like PointNet~\cite{qi2017pointnet} and PointNet++~\cite{qi2017pointnet++} introduced novel approaches for processing unordered point sets, employing symmetric functions to aggregate features. These methods laid the groundwork for subsequent architectures, including DGCNN~\cite{wang2019dynamic}, which incorporated edge features and local connectivity, and Point Transformer~\cite{zhao2021point}, which adapted transformer mechanisms for point cloud processing.

Recent advancements in self-supervised pre-trained models for point cloud data have gained considerable attention due to their strong performance across a range of computer vision tasks. These models are typically trained on extensive unlabeled datasets and later fine-tuned for specific downstream applications. Point cloud pre-training methods can be broadly categorized into three primary approaches: contrastive learning, reconstruction-based learning, and hybrid approaches that integrate both.

In contrastive learning, models like PointContrast~\cite{xie2020pointcontrast} and CrossPoint~\cite{afham2022crosspoint} capture semantic priors by comparing different perspectives of a unified point cloud, effectively learning discriminative features. Reconstruction-based methods, such as PointBERT~\cite{yu2022point} and PointMAE~\cite{pang2022masked}, draw inspiration from NLP and vision models by employing masked prediction and autoencoding frameworks. For instance, PointBERT adapts BERT-style masked patch prediction with a dVAE-based tokenizer for point clouds, while PointMAE reconstructs masked point coordinates through an autoencoder. To mitigate the challenge of limited labeled 3D data, ACT~\cite{dong2022autoencoders} utilizes a cross-modal teacher-student framework. It transfers knowledge from a fine-tuned cross-modal pre-trained teacher to a 3D point cloud learner, enabling the latter to acquire rich semantic representations through distillation.

\lr{add Graph-Based Methods, e.g., DGCNN, GraphTER; Transformer-Based Methods, e.g., Point Transformer(https://arxiv.org/pdf/2012.09164), Point-MAE. and compare the main difference of these methods}

Traditionally, these 3D pre-trained models are fine-tuned for downstream tasks through full parameter updates. However, full fine-tuning can be inefficient, often resulting in the degradation of valuable knowledge gained during pre-training and increasing the risk of catastrophic forgetting. This paper, therefore, investigates more efficient and effective strategies for transferring 3D pre-trained models to downstream tasks while preserving the benefits of pre-training.

\subsection{Parameter Efficient Fine-tuning}
Fine-tuning pre-trained models often demands substantial computational and storage resources. To address these limitations, parameter-efficient fine-tuning (PEFT) techniques have been developed, particularly within natural language processing (NLP) and computer vision. PEFT methods aim to transfer knowledge from pre-trained models to downstream tasks with minimal parameter updates. 
Adapter-based methods~\cite{houlsby2019parameter, hu2021lora, chen2022adaptformer}, e.g., insert lightweight modules into frozen backbone models, allowing adjustments without modifying the entire architecture. AdaptFormer~\cite{chen2022adaptformer} adds adapters parallel to feed-forward networks (FFNs) in visual recognition models. 
Prompt-based approaches~\cite{li2021prefix, jia2022visual} introduce a small number of learnable parameters to the input sequence, as seen in VPT-Deep~\cite{jia2022visual}, which adds trainable tokens at each layer’s input. 
Ladder Side Tuning (LST)~\cite{sung2022lst} takes a different route, adding side branches that leverage intermediate activations from the pre-trained model for enhanced prediction. However, applying these PEFT techniques directly to 3D point clouds has often yielded suboptimal results.

In recent years, PEFT methods tailored to 3D point cloud models have shown potential for improved performance. 
For instance, IDPT~\cite{zha2023instance} introduced the first PEFT approach specifically for point clouds, using DGCNN~\cite{wang2019dynamic} to generate instance-level prompts, effectively adapting traditional prompt-based tuning methods.
Point-PEFT~\cite{tang2024point} generates point prior prompts by constructing a domain-specific memory bank, while employing geometry-aware adapters to aggregate point cloud features within local spatial neighborhoods, capturing fine-grained geometric structures through local feature interactions. DAPT~\cite{zhou2024dynamic} introduces dynamic adapters to generate a dynamic scaling for each token conditioned on their importance to downstream tasks, along with internal prompts constructed to capture instance-specific features and facilitate adaptive feature interaction. PPT~\cite{zhang2024positional} adopts an encoded sampling center, combined with trainable position embeddings as additional prompt as additional prompt tokens to effectively inject the 3D spatial information of the point cloud into the  subsequent  network, progressively refined through the integration of lightweight adapters inserted between Transformer layers. Despite reducing training costs, these methods still struggle with consistent performance across tasks, as freezing pre-trained feature layers can limit the ability to capture fine-grained local features. Additionally, these methods often fall short in integrating local and global information, e.g., a critical component for dense prediction tasks~\cite{chen2022vitadapter}. PointLoRA~\cite{wang2025pointlora} integrates LoRA~\cite{hu2021lora} into the parameter-intensive components of point cloud pretrained models to capture global context, while employing a multi-scale token selection module to extract critical local information as prompts. PointGST~\cite{liang2024parameter}, developed concurrently with our work, shifts attention to the frequency domain by utilizing global and local spectral bases derived from the original point cloud to encode compact and intrinsic multi-scale representations. These are injected into intermediate features to enhance both global and local context. 

Most existing methods (e.g., PointGST~\cite{liang2024parameter}) rely on backbone networks to extract single-resolution features. However, pretrained backbones often excessively downsample the original point cloud (e.g., from 2048 points to 128) to generate point tokens. While this strategy improves computational efficiency, it inevitably discards a substantial amount of local structural information, leading to suboptimal performance in dense prediction tasks such as semantic segmentation, which require fine-grained spatial understanding. To mitigate this issue, we propose Point Ladder Tuning (PLT), a method that integrates a Hierarchical Local Network (HLN) to extract fine-grained, multi-resolution local features directly in the spatial domain, which are then adaptively fused with global features that are effectively captured by the well-pretrained backbone network through a LGF module to encode comprehensive contextual information and generate rich multi-scale features. This dual-domain architecture enables PLT to retain essential local details while leveraging powerful global priors, thereby achieving more comprehensive scene understanding. Furthermore, PLT maintains high efficiency, requiring significantly fewer trainable parameters, yet consistently delivering superior performance, particularly on challenging dense prediction benchmarks. \lr{solve what kind of problems in PointGST? Or, what is the remaining problem in PointGST and then we solve}
