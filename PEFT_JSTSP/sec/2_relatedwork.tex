\section{Related Work}
\label{sec:relatedwork}
The field of point cloud analysis has witnessed significant advancements in recent years, largely driven by the development of self-supervised pre-trained models and fine-tuning techniques. We give a review for pre-trained models and fine-tuning technologies for point cloud analysis.

\subsection{Pre-trained Models for Point Cloud}
Recent advancements in self-supervised pre-trained models for point cloud data have gained considerable attention due to their strong performance across a range of computer vision tasks. These models are typically trained on extensive unlabeled datasets and later fine-tuned for specific downstream applications. Point cloud pre-training methods can be broadly categorized into three primary approaches: contrastive learning, reconstruction-based learning, and hybrid approaches that integrate both.

In contrastive learning, models like PointContrast~\cite{xie2020pointcontrast} and CrossPoint~\cite{afham2022crosspoint} capture semantic priors by comparing different perspectives of a unified point cloud, effectively learning discriminative features. Reconstruction-based methods, such as PointBERT~\cite{yu2022point} and PointMAE~\cite{pang2022masked}, draw inspiration from NLP and vision models by employing masked prediction and autoencoding frameworks. For instance, PointBERT adapts BERT-style masked patch prediction with a dVAE-based tokenizer for point clouds, while PointMAE reconstructs masked point coordinates through an autoencoder. Hybrid methods like ReCon~\cite{qi2023contrast} combine both contrastive and reconstruction objectives, leveraging multi-modal inputs such as images and text to enrich feature learning during pre-training.

\lr{add Graph-Based Methods, e.g., DGCNN, GraphTER; Transformer-Based Methods, e.g., Point Transformer(https://arxiv.org/pdf/2012.09164), Point-MAE. and compare the main difference of these methods}

Traditionally, these 3D pre-trained models are fine-tuned for downstream tasks through full parameter updates. However, full fine-tuning can be inefficient, often resulting in the degradation of valuable knowledge gained during pre-training and increasing the risk of catastrophic forgetting. This paper, therefore, investigates more efficient and effective strategies for transferring 3D pre-trained models to downstream tasks while preserving the benefits of pre-training.

\subsection{Parameter Efficient Fine-tuning}
Fine-tuning pre-trained models often demands substantial computational and storage resources. To address these limitations, parameter-efficient fine-tuning (PEFT) techniques have been developed, particularly within natural language processing (NLP) and computer vision. PEFT methods aim to transfer knowledge from pre-trained models to downstream tasks with minimal parameter updates. 
Adapter-based methods~\cite{houlsby2019parameter, hu2021lora, chen2022adaptformer}, e.g., insert lightweight modules into frozen backbone models, allowing adjustments without modifying the entire architecture. AdaptFormer~\cite{chen2022adaptformer} adds adapters parallel to feed-forward networks (FFNs) in visual recognition models. 
Prompt-based approaches~\cite{li2021prefix, jia2022visual} introduce a small number of learnable parameters to the input sequence, as seen in VPT-Deep~\cite{jia2022visual}, which adds trainable tokens at each layerâ€™s input. 
Ladder Side Tuning (LST)~\cite{sung2022lst} takes a different route, adding side branches that leverage intermediate activations from the pre-trained model for enhanced prediction. However, applying these PEFT techniques directly to 3D point clouds has often yielded suboptimal results.

In recent years, PEFT methods tailored to 3D point cloud models have shown potential for improved performance. 
For instance, IDPT~\cite{zha2023instance} introduced the first PEFT approach specifically for point clouds, using DGCNN~\cite{wang2019dynamic} to generate instance-level prompts, effectively adapting traditional prompt-based tuning methods.
Point-PEFT~\cite{tang2024point} generates point prior prompts by constructing a domain-specific memory bank, while employing geometry-aware adapters to aggregate point cloud features within local spatial neighborhoods, capturing fine-grained geometric structures through local feature interactions. DAPT~\cite{zhou2024dynamic} introduces dynamic adapters to generate a dynamic scaling for each token conditioned on their importance to downstream tasks, along with internal prompts constructed to capture instance-specific features and facilitate adaptive feature interaction. PPT~\cite{zhang2024positional} adopts an encoded sampling center, combined with trainable position embeddings as additional prompt as additional prompt tokens to effectively inject the 3D spatial information of the point cloud into the  subsequent  network, progressively refined through the integration of lightweight adapters inserted between Transformer layers. Despite reducing training costs, these methods still struggle with consistent performance across tasks, as freezing pre-trained feature layers can limit the ability to capture fine-grained local features. Additionally, these methods often fall short in integrating local and global information, e.g., a critical component for dense prediction tasks~\cite{chen2022vitadapter}. PointGST~\cite{liang2024parameter}, developed concurrently with our work, shifts attention to the frequency domain by utilizing global and local spectral bases derived from the original point cloud to encode compact and intrinsic multi-scale representations. These are injected into intermediate features to enhance both global and local context. 

In contrast to PointGST~\cite{liang2024parameter}, we propose Point Ladder Tuning (PLT), a method that integrates a Hierarchical Local Network (HLN) to extract fine-grained local features in the spatial domain, which are then adaptively fused with global features that are effectively captured by the well-pretrained backbone network through a LGF module to encode comprehensive contextual information and generate rich multi-scale features. This dual-domain design minimizes the number of trainable parameters while achieving notable improvements in model performance, particularly on downstream tasks such as semantic segmentation. \lr{solve what kind of problems in PointGST? Or, what is the remaining problem in PointGST and then we solve}
