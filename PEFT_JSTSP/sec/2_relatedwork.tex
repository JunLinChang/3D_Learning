\section{Related Work}
\label{sec:relatedwork}

%Recent advances in point cloud analysis have been driven by deep learning methods, including both supervised architectures and self-supervised pre-training techniques. Meanwhile, Parameter-Efficient Fine-Tuning (PEFT) approaches, originally developed for NLP and vision, have shown promise in adapting large pre-trained models with minimal parameter updates. 
In this section, we review representative methods in both areas: deep learning techniques for 3D point cloud processing and the evolution of PEFT strategies across domains.

\subsection{Deep Learning on Point Cloud}

% add more details for these methods, e.g., DGCNN, PointNet++, Point Transformer, etc.
Deep learning on point clouds has evolved significantly, with various architectures designed to capture the unique characteristics of 3D data. Early methods like PointNet~\cite{qi2017pointnet} and PointNet++~\cite{qi2017pointnet++} introduced novel approaches for processing unordered point sets, employing symmetric functions to aggregate features. These methods laid the groundwork for subsequent architectures, including DGCNN~\cite{wang2019dynamic}, which incorporated edge features and local connectivity, and Point Transformer~\cite{zhao2021point}, which adapted transformer mechanisms for point cloud processing.

%Recent advancements in self-supervised pre-trained models for point cloud data have gained considerable attention due to their strong performance across a range of computer vision tasks. These models are typically trained on extensive unlabeled datasets and later fine-tuned for specific downstream applications. Point cloud pre-training methods can be broadly categorized into three primary approaches: contrastive learning, reconstruction-based learning, and hybrid approaches that integrate both.
Recent self-supervised pre-training methods for point clouds have shown strong performance across vision tasks. These models train on large unlabeled datasets before fine-tuning for specific applications. Current approaches mainly fall into three categories: contrastive learning, reconstruction-based learning, and hybrid methods combining both.
In contrastive learning, models like PointContrast~\cite{xie2020pointcontrast} and CrossPoint~\cite{afham2022crosspoint} capture semantic priors by comparing different perspectives of a unified point cloud, effectively learning discriminative features. 
Reconstruction-based methods, such as PointBERT~\cite{yu2022point} and PointMAE~\cite{pang2022masked}, draw inspiration from NLP and vision models by employing masked prediction and autoencoding frameworks. For instance, PointBERT adapts BERT-style masked patch prediction with a dVAE-based tokenizer for point clouds, while PointMAE reconstructs masked point coordinates through an autoencoder. 
To mitigate the challenge of limited labeled 3D data, ACT~\cite{dong2022autoencoders} utilizes a cross-modal teacher-student framework. It transfers knowledge from a fine-tuned cross-modal pre-trained teacher to a 3D point cloud learner, enabling the latter to acquire rich semantic representations through distillation.

Traditionally, these 3D pre-trained models are fine-tuned for downstream tasks through full parameter updates. However, full fine-tuning can be inefficient, often resulting in the degradation of valuable knowledge gained during pre-training and increasing the risk of catastrophic forgetting. Therefore, this paper investigates more efficient and effective strategies for transferring 3D pre-trained models to downstream tasks while preserving the benefits of pre-training.

\subsection{Parameter Efficient Fine-tuning}
Fine-tuning pre-trained models often demands substantial computational and storage resources. To address these limitations, parameter-efficient fine-tuning (PEFT) techniques have been developed, particularly within natural language processing (NLP) and computer vision. PEFT methods aim to transfer knowledge from pre-trained models to downstream tasks with minimal parameter updates. 
Adapter-based methods~\cite{houlsby2019parameter, hu2021lora, chen2022adaptformer}, e.g., insert lightweight modules into frozen backbone models, allowing adjustments without modifying the entire architecture. AdaptFormer~\cite{chen2022adaptformer} adds adapters parallel to feed-forward networks (FFNs) in visual recognition models. 
Prompt-based approaches~\cite{li2021prefix, jia2022visual} introduce a small number of learnable parameters to the input sequence, as seen in VPT-Deep~\cite{jia2022visual}, which adds trainable tokens at each layer’s input. 
Ladder Side Tuning (LST)~\cite{sung2022lst} takes a different route, adding side branches that leverage intermediate activations from the pre-trained model for enhanced prediction. However, applying these PEFT techniques directly to 3D point clouds has often yielded suboptimal results.

In recent years, PEFT methods tailored to 3D point cloud models have shown potential for improved performance. 
For instance, IDPT~\cite{zha2023instance} introduced the first PEFT approach specifically for point clouds, using DGCNN~\cite{wang2019dynamic} to generate instance-level prompts, effectively adapting traditional prompt-based tuning methods.
Point-PEFT~\cite{tang2024point} generates point prior prompts by constructing a domain-specific memory bank, while employing geometry-aware adapters to aggregate point cloud features within local spatial neighborhoods, capturing fine-grained geometric structures through local feature interactions. DAPT~\cite{zhou2024dynamic} introduces dynamic adapters to generate a dynamic scaling for each token conditioned on their importance to downstream tasks, along with internal prompts constructed to capture instance-specific features and facilitate adaptive feature interaction. PPT~\cite{zhang2024positional} encodes sampling centers as trainable position prompt tokens to inject 3D spatial information, progressively refined through lightweight adapters placed between Transformer layers. Despite reducing training costs, these methods still struggle with consistent performance across tasks, as freezing pre-trained feature layers can limit the ability to capture fine-grained local features. Additionally, They often fall short in integrating local and global information, which is a critical component for dense prediction tasks~\cite{armeni20163d}. PointLoRA~\cite{wang2025pointlora} integrates LoRA~\cite{hu2021lora} into the parameter-intensive components of point cloud pretrained models to capture global context, while employing a multi-scale token selection module to extract critical local information as prompts. PointGST~\cite{liang2024parameter}, developed concurrently with our work, shifts attention to the frequency domain by utilizing global and local spectral bases derived from the original point cloud to encode compact and intrinsic multi-scale representations. These are injected into intermediate features to enhance both global and local context. 

Most existing methods (e.g., PointGST~\cite{liang2024parameter}) extract single-resolution features via the backbone's heavily downsampled point tokens (e.g., from 2048 to 128), which improves efficiency but discards fine-grained local structure—crucial for dense prediction tasks like semantic segmentation. To address this issue, we propose Point Ladder Tuning (PLT), which integrates a Hierarchical Local Network (HLN) to extract multi-resolution local features directly in the spatial domain. These features are then adaptively fused with global representations from the frozen backbone via a Local-Global Fusion (LGF) module. This dual-domain architecture preserves essential local details while leveraging rich global priors, achieving more comprehensive scene understanding with fewer trainable parameters and superior performance, particularly on challenging dense prediction benchmarks.
