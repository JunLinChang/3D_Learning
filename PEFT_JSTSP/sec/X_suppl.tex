\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\appendix

\section{Dataset and Training Detail}

All experiments were conducted using a single GeForce RTX 3090 GPU. During the training phase, the weights of the pre-trained model were kept fixed, and only a limited subset of parameters associated with the added modules were fine-tuned. The detailed hyperparameter configurations of the model are presented in Table~\ref{tab:paramas}

\textbf{ScanObjectNN}~\cite{uy2019revisiting} comprises approximately 15,000 point cloud samples distributed across 15 categories, collected from indoor scenes that frequently include background noise and occlusions. The dataset provides three commonly used variants: OBJ\_BG, OBJ\_ONLY, and PB\_T50\_RS, with progressively increasing levels of difficulty. The training configuration adheres to the settings used for baseline comparisons. Specifically, the AdamW optimizer is utilized with an initial learning rate of $5 \times 10^{-4}$ and a weight decay of 0.05. A cosine learning rate scheduler with a 10-epoch warm-up period is employed. The model is trained for a total of 300 epochs with a batch size of 32. To ensure consistency, each point cloud is sampled to 2048 points, which are partitioned into 128 groups, each containing 32 points, within the Patch Encoder.

\textbf{ModelNet}~\cite{wu20153d} consists of 12,311 clean 3D CAD models across 40 categories. The input point clouds are sampled to 1,024 points, which are organized into 64 groups within the Patch Encoder, with each group containing 32 points. Under the full-sample training configuration, the network settings are consistent with those used for ScanObjectNN. For the few-shot experiments, the model is trained for 150 epochs.

\textbf{ShapeNetPart}~\cite{yi2016scalable} is a widely used benchmark for point-level synthetic object part segmentation, comprising 16,881 samples distributed across 16 object categories and 50 part categories. Training is performed using the AdamW optimizer with a weight decay of 0.05 and a cosine learning rate scheduler. The initial learning rate is set to $2 \times 10^{-4}$, with a warm-up period of 10 epochs. The models are trained for 300 epochs using a batch size of 16. Each input point cloud contains 2,048 points, which are grouped into 128 clusters, with each cluster containing 32 points.

\textbf{S3DIS}~\cite{armeni20163d} is a large-scale indoor scene dataset comprising six areas with a total of 273 million points annotated across 13 categories. Following established practices~\cite{dong2022autoencoders}, Area 5 is recommended for evaluation to provide a more reliable and standardized assessment of performance in semantic segmentation tasks. The model is trained using a cosine learning rate scheduler with an initial learning rate of $2 \times 10^{-4}$. Training is performed over 60 epochs with a batch size of 32, while other configurations are consistent with those used for ShapeNetPart~\cite{yi2016scalable}.


\section{Additianal Experiments}

\subsection{Performance Analysis}  
As shown in Fig.~\ref{fig:performance}, we evaluated the speed and memory usage of our proposed PLT approach in comparison with previous methods during both training and inference. In Fig.~\ref{fig:per1}, it can be observed that, with smaller batch sizes, the training speed of our method is initially slower than that of full fine-tuning and even falls below that of IDPT~\cite{zha2023instance}. However, as the batch size increases, the training speed of our approach surpasses that of full fine-tuning. Specifically, at a batch size of 128, our method achieves a training speed nearly 30\% faster than IDPT~\cite{zha2023instance}. Fig.~\ref{fig:per2} demonstrates that our method consistently requires less memory for training compared to full fine-tuning. By contrast, both IDPT~\cite{zha2023instance} and Point-PEFT~\cite{tang2024point} exhibit greater memory usage than full fine-tuning. Notably, at a batch size of 256, our approach reduces memory consumption by 1.2 GB relative to full fine-tuning. Regarding inference speed, as depicted in Fig.~\ref{fig:per3}, our method consistently outperforms IDPT~\cite{zha2023instance} and Point-PEFT~\cite{tang2024point}, achieving performance close to, albeit slightly below, that of full fine-tuning. Additionally, Fig.~\ref{fig:per4} indicates that the difference in memory usage during inference between our method and full fine-tuning is minimal. Conversely, IDPT~\cite{zha2023instance} and Point-PEFT~\cite{tang2024point} exhibit substantially higher memory usage, particularly at a batch size of 512.

\subsection{Attention Score of LGF}  
As illustrated in Fig.~\ref{fig:attention_score}, the local attention scores of LGF across all categories and layers remain consistently below 0.5 on the most challenging variant of ScanObjectNN~\cite{uy2019revisiting} (i.e., PB\_T50\_RS). This underscores the pivotal role of pretrained global prior information in facilitating downstream task learning. Furthermore, the attention scores exhibit category-dependent variations, with more pronounced differences in shallow layers and greater uniformity in deeper layers. This observation suggests that the integration of local and global information is highly specific to different categories in the shallow layers, while becoming more generalized in the deeper layers. Additionally, the attention scores for local information tend to decline progressively from shallow to deeper layers across most categories. This trend indicates that the network prioritizes learning local information in the early layers, while progressively shifting its focus toward global information in the later stages.

\subsection{Token Selections for Head Inputs}  
We investigated the influence of various input tokens on the downstream task heads, including class token, the pooling of prompt tokens, the pooling of point patch tokens, and the pooling of HLN output tokens. As shown in Fig.~\ref{fig:head}, we evaluated five standard token selection strategies. The results indicate that the highest performance is achieved when all four token types are utilized. Notably, adding the pooling of prompt tokens to the class token, rather than concatenating them, improves accuracy by 0.55\% while reducing the number of trainable parameters. Furthermore, we observed that removing the point patch tokens results in a more substantial decline in performance compared to the removal of either the prompt tokens or HLN output tokens. This underscores the essential role of prior information from pre-trained models in enabling high-quality performance on downstream tasks.

\subsection{Qualitative Analysis}

\subsubsection{t-SNE Visualizations}  
Fig.~\ref{fig:tsne} presents the t-SNE~\cite{van2008visualizing} visualizations of feature manifolds generated by fully fine-tuned methods, point-cloud-specific fine-tuning approaches such as IDPT~\cite{zha2023instance} and DAPT~\cite{zhou2024dynamic}, and our proposed PLT, all trained on the ScanObjectNN PB\_T50\_RS dataset~\cite{uy2019revisiting}. The dispersion of points between categories in the t-SNE plot reflects the quality of the model's feature representationâ€”greater inter-cluster dispersion and smaller intra-cluster average distance indicate stronger discriminative power, facilitating easier classification.

Notably, our PLT demonstrates significantly greater inter-cluster dispersion and a smaller intra-cluster average distance compared to previous methods. This suggests that our approach effectively enhances feature separation between categories while maintaining compactness within each category, indicative of a superior ability to learn and represent discriminative features.

Furthermore, PLT achieves these results while using fewer learnable parameters, showcasing its efficiency in leveraging pre-trained models for adaptation to downstream tasks. This not only reduces computational cost but also highlights PLT's potential for broader applicability in resource-constrained environments. In comparison to fully fine-tuned and point-cloud-specific methods, PLT strikes an optimal balance between parameter efficiency and representation quality, underscoring its value in point cloud learning tasks.

\input{fig/supplement/attention_score}

\subsubsection{Part Segmentation Visualizations}  
Fig.~\ref{fig:part_segmentation} provides a visual demonstration of the part segmentation results obtained using our proposed PLT, based on the PointMAE~\cite{pang2022masked} baseline. We select five representative samples from various categories and render three distinct perspectives for each sample. The figure clearly illustrates that our method delivers exceptional segmentation performance. Notably, the PLT method achieves high accuracy in distinguishing object parts with remarkable precision, while maintaining efficiency by using a minimal set of parameters.

\subsubsection{Semantic Segmentation Visualizations}  
Fig.~\ref{fig:semantic} presents a qualitative comparison of segmentation results between our proposed PLT method and prior fine-tuning approaches in semantic segmentation tasks on Area 5 of the S3DIS dataset~\cite{armeni20163d}. The comparison highlights the advantages of PLT in addressing common challenges in point cloud segmentation. From the first row of the figure, it is evident that while full fine-tuning is resource-intensive, it struggles to accurately segment object boundaries. This often results in poor boundary delineation and misclassification in continuous regions, primarily due to insufficient capture of local point cloud information. A more detailed comparison in the office\_9 scene reveals that methods such as full fine-tuning, IDPT~\cite{zha2023instance}, and DAPT~\cite{zhou2024dynamic} incorrectly classify walls as beams and cluttered regions as ceilings. In contrast, our PLT method successfully segments clutter regions while significantly reducing the misclassification of walls. Similarly, in the office\_35 scene, PLT outperforms other methods in segmenting challenging areas such as the cluttered region and the board. Whereas methods like full fine-tuning, IDPT~\cite{zha2023instance}, and PPT~\cite{zhang2024positional} fail to accurately recognize or segment these regions, PLT demonstrates superior performance, maintaining clarity and precision. These results emphasize the ability of the PLT method to deliver outstanding segmentation performance while using far fewer fine-tuning parameters. Compared to other approaches, PLT not only provides clearer segmentation boundaries but also significantly reduces misclassifications, further validating its efficiency and effectiveness for point cloud semantic segmentation tasks.


\input{tab/appendix/parames}

\input{fig/tsne}

\input{fig/head}
\input{fig/supplement/performance}

\input{fig/supplement/part_segmentation}

\input{fig/supplement/semantic_segmentation}
