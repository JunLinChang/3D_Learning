\setcounter{page}{1}
%\appendix

\section{Dataset and Training Detail}

All experiments were conducted using a single GeForce RTX 3090 GPU. During the training phase, the weights of the pre-trained model were kept fixed, and only a limited subset of parameters associated with the added modules were fine-tuned. The detailed hyperparameter configurations of the model are presented in Table~\ref{tab:paramas}

\input{tab/appendix/parames}


\section{Additianal Experiments}

\subsection{Object Classification on Synthetic Dataset}
We conduct experiments on the ModelNet dataset~\cite{wu20153d}, which contains 12,311 clean 3D CAD models spanning 40 categories. Following the protocol in DAPT~\cite{zhou2024dynamic}, we split ModelNet40 into 9,843 training samples and 2,468 testing samples. During training, we applied standard data augmentation techniques, including random scaling and random translation. The training configuration adheres to the settings used for baseline comparisons. Specifically, the AdamW optimizer is utilized with an initial learning rate of $5 \times 10^{-4}$ and a weight decay of 0.05. A cosine learning rate scheduler with a 10-epoch warm-up period is employed. The model is trained for a total of 300 epochs with a batch size of 32. 

Without voting, PLT achieves accuracy rates of 93.8\%, 93.5\% and 93.6\% in Point-MAE, Point-BERT and ACT respectively, representing gains of \textbf{0.6\%}, \textbf{0.8\%} and \textbf{0.6\%} over full fine-tuning. When using voting, PLT continues to outperform full fine-tuning, notably improving Point-BERT's accuracy by an additional \textbf{1.0\%}. These results demonstrate the effectiveness of PLT for point cloud classification, as it explicitly models local structures on the original point cloud while jointly leveraging global and local representations to enhance performance.

\subsection{Token Selections for Head Inputs}  
To better understand the role of different token representations in downstream classification, we conduct an ablation study that systematically evaluates five input configurations for the classification head. Specifically, we investigate the impact of incorporating the class token $T_{cls}$, prompt tokens, patch tokens from the frozen backbone, and tokens from the HLN output. As illustrated in Fig.~\ref{fig:head}, each configuration corresponds to a different combination of these token sources.

Our results show that the best performance (85.53\% overall accuracy) is achieved when all four token types are utilized together (Fig.\ref{fig:head}(a)), demonstrating the complementary nature of global semantics and local geometric cues. Interestingly, pooling the prompt tokens and fusing them with the class token, as opposed to directly concatenating all tokens, yields a performance improvement of 0.55\% (comparing Fig.\ref{fig:head}(a) and (b)), while also reducing the number of trainable parameters, suggesting more efficient use of contextual information.

Furthermore, we observe that removing patch tokens leads to the most significant performance degradation (Fig.~\ref{fig:head}(e)), compared to the exclusion of either prompt tokens or HLN tokens. This finding underscores the crucial role of backbone-derived features, which encapsulate strong prior knowledge learned from large-scale pretraining, in ensuring robust downstream performance.

\subsection{Qualitative Analysis}

\subsubsection{t-SNE Visualizations}  
To complement the t-SNE results discussed in the main text, Fig.\ref{fig:tsne1} provides a comprehensive visualization of the feature manifolds learned by various fine-tuning strategies on the ScanObjectNN PB\_T50\_RS benchmark\cite{uy2019revisiting}, using t-SNE~\cite{van2008visualizing}. The evaluated strategies include full fine-tuning, linear probing, point-cloud-specific adaptation methods, and our proposed PLT approach. We extract the final-layer classification features and project them into a 2D space using t-SNE to facilitate visual interpretation. As illustrated in Fig.~\ref{fig:tsne1}, the features produced by PLT form more compact and well-separated clusters than those generated by both conventional and recent PEFT baselines. This suggests stronger class discrimination and improved feature organization. Importantly, PLT achieves these results with substantially fewer trainable parameters than full fine-tuning, underscoring its efficiency and effectiveness in adapting pre-trained models to downstream 3D recognition tasks.

\subsubsection{Part Segmentation Visualizations}  
Fig.~\ref{fig:part_segmentation} provides a visual demonstration of the part segmentation results obtained using our proposed PLT, based on the PointMAE~\cite{pang2022masked} baseline. We select five representative samples from various categories and render three distinct perspectives for each sample. The figure clearly illustrates that our method delivers exceptional segmentation performance. Notably, the PLT method achieves high accuracy in distinguishing object parts with remarkable precision, while maintaining efficiency by using a minimal set of parameters.

\subsubsection{Semantic Segmentation Visualizations}  
To complement the semantic segmentation visualization results discussed in the main text, Fig.\ref{fig:s3dis_1} presents qualitative comparisons between our proposed PLT method and several existing fine-tuning approaches on Area 5 of the S3DIS dataset\cite{armeni20163d}. These results highlight PLTâ€™s effectiveness in addressing key challenges in point cloud segmentation, particularly in preserving structural boundaries and reducing misclassifications. From the first row of the figure, it is evident that while full fine-tuning is resource-intensive, it struggles to accurately segment object boundaries. This often results in poor boundary delineation and misclassification in continuous regions, primarily due to insufficient capture of local point cloud information. A more detailed comparison in the office\_9 scene reveals that methods such as full fine-tuning, IDPT~\cite{zha2023instance}, and DAPT~\cite{zhou2024dynamic} incorrectly classify walls as beams and cluttered regions as ceilings. In contrast, our PLT method successfully segments clutter regions while significantly reducing the misclassification of walls. Similarly, in the office\_35 scene, PLT outperforms other methods in segmenting challenging areas such as the cluttered region and the board. Whereas methods like full fine-tuning, IDPT~\cite{zha2023instance}, and PPT~\cite{zhang2024positional} fail to accurately recognize or segment these regions. In the wc\_2 scene, most methods misclassify cluttered region into door, but our PLT can accurately distinguish them. These results emphasize the ability of the PLT method to deliver outstanding segmentation performance while using far fewer fine-tuning parameters. Compared to other approaches, PLT not only provides clearer segmentation boundaries but also significantly reduces misclassifications, further validating its efficiency and effectiveness for point cloud semantic segmentation tasks.

\input{fig/head}

\input{fig/tsne1}

\input{fig/supplement/part_segmentation}

\input{fig/supplement/semantic_segmentation}
