\section{Introduction}
\label{sec:intro}
\input{fig/sota}

The widespread adoption of 3D scanning technology has propelled point cloud learning into a critical research area, with applications spanning autonomous driving~\cite{yang2024visual,song2024graphbev,chen20203d}, VR/AR~\cite{casado2023rendering,garrido2021point}, and robotics~\cite{wang2021trajectory,chen2022direct,christen2023learning}. However, unlike structured image data defined on dense grids, point clouds exhibit three fundamental properties that shape their feature learning: (1) Sparsity (non-uniform sampling with large empty spaces), (2) Unstructuredness (lack of explicit neighborhood relationships), and (3) Permutation invariance (order-agnostic point sets). These properties demand architectures that can dynamically model local geometric structures while preserving global consistency—a challenge unmet by conventional grid-based deep learning paradigms. While prior work has developed specialized architectures~\cite{qi2017pointnet, li2018pointcnn, qian2022pointnext, qi2017pointnet++, wang2019dynamic, wu2024point} to address these challenges, the prevailing paradigm for adapting pre-trained models, full fine-tuning, remains computationally wasteful and often unnecessary.

In this work, we challenge this convention by demonstrating that high performance can be achieved by updating only a minimal subset of parameters while freezing the bulk of the pre-trained backbone. Our findings reveal two key insights:
(1) Early layers in point cloud networks capture general geometric priors that remain highly transferable across tasks, eliminating the need for exhaustive re-training.
(2) Task-specific adaptation is most effective when focused on local structural cues, rather than global feature alignments, enabling efficient learning with minimal parameter updates.

To implement these insights, we introduce a hierarchical local-global fusion (HLGF) mechanism that bridges frozen backbone features with lightweight, task-specific adaptations. This approach allows diverse downstream tasks (e.g., classification, segmentation) to share a single frozen backbone, achieving competitive performance with $<$10$\%$ of the tuned parameters required by conventional fine-tuning.


%The growing accessibility of 3D scanning technology has elevated 3D point cloud learning to an emerging research area with diverse applications across computer vision and graphics fields, including autonomous driving (add citation), virtual and augmented reality (VR/AR) (add citation), and robotics (add citation).  
%Unlike images, point clouds are inherently unstructured, sparse, and permutation-invariant, which poses unique challenges for effective analysis and processing. 
%Consequently, deep learning-based methods~\cite{qi2017pointnet, li2018pointcnn,qian2022pointnext,qi2017pointnet++,wang2019dynamic,wu2024point,wu2022point,zhang2022patchformer,park2022fast,zhao2021point,guo2021pct,ma2022rethinking} specifically tailored for point cloud learning have been developed, incorporating specialized modules to directly handle point cloud data and achieving substantial improvements in performance.

%The widespread availability of 3D scanning technology has elevated 3D point cloud learning into a rapidly growing research field with applications across computer vision and graphics, notably in autonomous driving~\cite{yang2024visual,song2024graphbev,chen20203d}, virtual and augmented reality (VR/AR)~\cite{casado2023rendering,garrido2021point}, and robotics~\cite{wang2021trajectory,chen2022direct,christen2023learning}. Unlike images, point clouds are inherently unstructured, sparse, and permutation-invariant, which introduces unique challenges for analysis and processing. Consequently, deep learning approaches specifically tailored for point cloud data~\cite{qi2017pointnet, li2018pointcnn, qian2022pointnext, qi2017pointnet++, wang2019dynamic, wu2024point} have been developed, achieving notable performance gains by incorporating modules designed to handle these challenges directly.

%\blk{Recently, inspired by the success of pre-trained models in natural language processing~\cite{devlin2018bert,brown2020language,2020ALBERT,raffel2020exploring,touvron2023llama,team2024gemma} and computer vision~\cite{he2020momentum,chen2020improved,chen2021empirical,he2022masked,xie2022simmim,yeh2022decoupled}, a series of works~\cite{pang2022masked,yu2022point,zhang2022point,afham2022crosspoint,dong2022autoencoders,liu2022masked,xie2020pointcontrast,qi2023contrast,wang2021unsupervised} have also emerged in the field of point cloud analysis.After pre-training, these methods employ a classic full fine-tuning strategy to adapt to downstream tasks, resulting in significant performance improvements and faster convergence compared to training from scratch.However, fully fine-tuning techniques may be suboptimal for point cloud processing for three reasons: 1) Updating all parameters of the pre-trained model can lead to overfitting and catastrophic forgetting, undermining the rich embedding knowledge acquired during the pre-training phase and resulting in poor performance. 2) Each point cloud analysis task and dataset requires separate copies of model parameters, which can pose storage challenges as demand increases. 3) To fully leverage the prior knowledge from the pre-training dataset, large models are often necessary. Full fine-tuning can incur significant computational costs, including increased GPU memory usage and extended training times, limiting accessibility for researchers with poor hardware resources.}

Inspired by the success of pre-trained models in natural language processing~\cite{devlin2018bert, brown2020language} and computer vision~\cite{he2020momentum, chen2020improved}, recent research has extended this paradigm to point cloud analysis~\cite{pang2022masked, yu2022point, zhang2022point, afham2022crosspoint} (\lr{inspire by what kind of specific technology}), where pre-trained models typically undergo full fine-tuning for downstream tasks. While effective, full fine-tuning for point clouds has limitations: (1) Updating all model parameters can lead to overfitting and catastrophic forgetting, diminishing the benefits of pre-trained embeddings. (2) Each task and dataset requires its own model, leading to storage inefficiencies as demand grows. (3) Full fine-tuning incurs high computational costs, including increased GPU memory usage and extended training times, which may limit accessibility for researchers with constrained hardware resources.

To address these limitations, our research explores Parameter-Efficient Fine-Tuning (PEFT)~\cite{houlsby2019parameter, jie2023fact, karimi2021compacter}, a strategy that has gained traction in NLP and vision by freezing most model parameters and allowing selective, efficient updates. PEFT approaches allow comparable or even improved performance over full fine-tuning while significantly reducing the number of parameter updates. Examples include (1) Adapters~\cite{houlsby2019parameter, chen2022adaptformer}, which insert lightweight networks into Transformer layers; (2) Prompt Tuning~\cite{li2021prefix, lester-etal-2021-power}, which adds learnable parameters to the input sequence; and (3) Ladder Side Tuning~\cite{sung2022lst}, which uses an independent Ladder network to refine intermediate representations. These techniques have shown promising performance in transferring pre-trained knowledge while maintaining computational efficiency.

However, our empirical analysis (see Sec.~\ref{sec:classification}) reveals that directly adapting fine-tuning strategies from language and vision models to point cloud data often results in sub-optimal performance. This observation highlight a critical question: How can we design a fine-tuning approach specifically optimized for point clouds that not only achieves but potentially surpasses the performance of full fine-tuning, while remaining efficient and effective? Addressing this challenge is essential for advancing parameter-efficient techniques tailored to the unique demands of 3D data.

In response, we introduce a novel fine-tuning approach for point cloud data, termed Point Ladder Tuning (PLT), which builds upon Ladder Side Tuning. While the backbone network’s attention mechanisms effectively capture global semantic information, which lacks the granularity needed for fine local detail. To overcome this, PLT incorporates a hierarchical Ladder Network (HLN) to directly extract local information from raw input, enhancing the detail available for downstream tasks. Furthermore, we propose a Local-Global Fusion (LGF) module, which adaptively combines local information from the Ladder Network with global features from the backbone network, generating rich multi-scale representations essential for improving performance.


To further optimize the pre-trained backbone for specific tasks, we introduce an adaptive prompt generation module. This module learns to scale and translate the fused features, producing instance-specific, multi-scale prompts that enable the backbone to refine its features with high efficiency and effectiveness.

Our primary contributions are as follows:

\begin{itemize} 
	\item We present Point Ladder Tuning (PLT), a parameter-efficient fine-tuning method for point cloud data that combines a hierarchical Ladder Network for local information extraction and a Local-Global Fusion (LGF) module for multi-scale feature integration. 
	\item We introduce a prompt generation module that linearly maps the output of the LGF module, injecting multi-scale information directly into the backbone network to further enhance performance. \item Extensive experiments demonstrate that PLT achieves performance comparable to, and often surpassing, full fine-tuning across various tasks and datasets, while requiring significantly fewer parameters. 
\end{itemize}



