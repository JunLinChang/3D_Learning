\begin{abstract}
Fine-tuning pre-trained models for point cloud analysis typically involves updating all model parameters, leading to substantial computational overhead. In this paper, we demonstrate that high performance can be achieved with a parameter-efficient approach that selectively updates only a minimal subset of parameters, focusing on local neighborhood information rather than exhaustive parameter updates and global attention mechanisms. Our method employs parameter-efficient transfer learning, optimizing task performance while minimizing parameter costs. Specifically, we freeze the parameters of conventional pre-trained models and introduce a Hierarchical Ladder Network (HLN) to directly capture essential local information from the input point cloud. To bridge this local information with the backbone’s intermediate global representations, we propose a Local-Global Fusion (LGF) module that integrates multi-scale features effectively. Additionally, our method generates dynamic prompts from these fused features to enhance the backbone’s capacity for producing optimized global features across diverse downstream tasks. Extensive experiments on point cloud classification and dense prediction tasks validate the effectiveness of our approach, which achieves superior performance with a fraction of the parameters required for full fine-tuning—using only 2.71\% to 7.69\% of the parameters for object classification and dense prediction, respectively. 

\end{abstract}