\section{Conclusion and Limitation}
\label{sec:conclusion&limitation}

In this paper, we introduced Point Ladder Tuning (PLT), a parameter-efficient fine-tuning strategy designed specifically for point cloud analysis. PLT freezes the parameters of the backbone network and employs a hierarchical Ladder Network (HLN) to extract local information directly from the input point cloud. To effectively integrate this local information with intermediate global features from the backbone network, we proposed a Local-Global Fusion (LGF) module. Furthermore, multi-scale fused features are leveraged to generate adaptive prompts, optimizing the backbone network's global features for downstream tasks. Our experiments demonstrate that PLT achieves competitive performance in tasks such as object classification and dense prediction, all while requiring minimal additional parameters. However, this study is limited to classification and segmentation tasks, leaving evaluations on other tasks, such as point cloud object detection and generation, as directions for future work.
