\section{Conclusion and Limitation}
\label{sec:conclusion&limitation}
In this study, we propose a parameter-efficient fine-tuning strategy, named PLT, for point cloud analysis. PLT effectively freezes the parameters of the backbone network and utilizes a hierarchical Ladder Network (HLN) to directly extract local information from the input point cloud. To aggregate this local information with the backbone networkâ€™s intermediate global activation, we introduce a Local-Global Fusion (LGF) module. Additionally, to further enhance performance, fused multi-scale features are used to generate dynamic prompts, enabling the backbone network to produce global features optimized for downstream tasks. Our method demonstrates impressive performance in tasks such as object classification and dense prediction, while maintaining a minimal parameter footprint. However, a limitation of this approach is that it has not yet been validated on additional tasks, such as point cloud object detection and point cloud generation, which we leave as future work.