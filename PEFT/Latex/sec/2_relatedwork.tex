\section{Related Work}
\label{sec:relatedwork}

\subsection{3D Pre-trained Models}
% \subsection{3D Shape Analysis}
% The unstructured, sparse, and disordered characteristics of point clouds pose a significant challenge in the field of computer vision. A groundbreaking solution to this problem is PointNet\cite{qi2017pointnet}\cite{wang2019dynamic,wang2018local,li2019deepgcns,zhang2022graph}, which utilizes shared multi-layer perceptrons (MLPs) to efficiently extract global features from point clouds. Subsequent developments, integrating advanced technologies such as graph learning\cite{wang2019dynamic,wang2018local,li2019deepgcns,zhang2022graph}, convolutional methods\cite{li2018pointcnn,wu2019pointconv}, self-attention mechanisms\cite{guo2021pct,zhao2021point,wu2024point}, and Mamba\cite{liang2024pointmamba,liu2024point,wang2024serialized}, have further promoted the progress of this field.

Recently, self-supervised pre-trained point cloud models have garnered significant attention due to their outstanding performance. These models are trained on vast amounts of unlabeled data and subsequently fine-tuned for various downstream tasks. Point cloud pre-training can be categorized into three main types: contrastive learning-based, reconstruction-based, and a combination of both. In contrastive learning, models like PointContrast\cite{xie2020pointcontrast} and CrossPoint\cite{afham2022crosspoint} leverage rich semantic priors by comparing and learning features from different perspectives of a unified point cloud. PointBERT\cite{yu2022point} adapts concepts from BERT\cite{devlin2018bert}, learning point cloud features by predicting masked patches and comparing them with the output features from dVAE-based point cloud tokenizers. PointMAE\cite{pang2022masked}, inspired by MAE\cite{he2022masked}, directly predicts the coordinates of the masked points using an autoencoder. Meanwhile, ReCon\cite{qi2023contrast} combines contrastive learning and mask reconstruction, integrating additional information such as images and text to enhance the quality of pre-training.

Currently, 3D pre-trained models are usually transferred to downstream tasks through full fine-tuning. However, this is often inefficient and may undermine the valuable prior knowledge obtained during pre-training, potentially leading to catastrophic forgetting. Therefore, in this paper, we mainly focus on strategies for efficiently and effectively transferring 3D pre-trained models to downstream tasks.

\subsection{Parameter Efficient Fine-tuning}

Fine-tuning pre-trained models can be costly in terms of computation and storage. To address this challenge, many researchers in NLP and computer vision have explored parameter-efficient fine-tuning techniques that enable the transfer of pre-trained knowledge to downstream tasks using only a minimal number of parameters. the Adapter-based methods\cite{houlsby2019parameter,hu2021lora,chen2022adaptformer} typically involve inserting lightweight networks into frozen backbone models to adjust the pre-trained architecture. For instance, AdaptFormer\cite{chen2022adaptformer} adds adapters in parallel to the feed-forward network (FFN) for visual recognition tasks.the Prompt-based methods\cite{li2021prefix, jia2022visual} generally incorporate a small number of learnable parameters into the input sequence. For example, VPT-Deep\cite{jia2022visual} inserts learnable parameters into the input of each layer. Different from the previous two methods, LST\cite{sung2022lst} introduces additional branches that utilize the intermediate activations of the pretrained model as inputs for prediction. However, simply applying these methods to the field of 3D point clouds does not yield satisfactory results.

Recently, several PEFT methods\cite{zha2023instance,tang2024point,zhou2024dynamic,liang2024parameter,zhang2024positional} designed for pre-training 3D point cloud models have shown significant performance improvements. IDPT\cite{zha2023instance} is the first PEFT method specifically tailored for point clouds, utilizing DGCNN\cite{wang2019dynamic} to generate instance-level prompts, effectively replacing traditional VPT. Point-PEFT\cite{tang2024point} employs adapters to aggregate local features during the fine-tuning process, while DAPT\cite{zhou2024dynamic} introduces dynamic adapters that assess the importance of each token in downstream tasks, generating dynamic weights for each token. Although these methods successfully reduce training costs, achieving consistent performance improvements across various tasks remains challenging. We believe that these approaches depend on freezing the features of pre-trained models as input, which hinders the learning of discriminative local features. Furthermore, they do not fully integrate the global and local information of point clouds, a critical aspect for dense prediction tasks\cite{chen2022vitadapter}. To this end, we propose Point Ladder Tuning(PLT), which utilizes a hierarchical Ladder network to directly extract local information from point cloud inputs and fuses it with the global features of the backbone network through attention to obtain multi-scale features. This method significantly reduces adjustable parameters and achieves remarkable performance.
