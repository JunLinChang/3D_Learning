\begin{abstract}
\blk{Point cloud analysis has achieved outstanding performance by transferring point cloud pre-trained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient and ineffective. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pre-trained models and utilizes a hierarchical Ladder Network (HLN) to directly extract local information from the input point cloud. To aggregate this local information with the backbone network’s intermediate global activation, we introduce a Local-Global Fusion (LGF) module. Additionally, to further enhance performance, fused multi-scale features are used to generate dynamic prompts, enabling the backbone network to produce global features optimized for downstream tasks. A series of experiments on tasks such as point cloud object classification and dense prediction have demonstrated the effectiveness of our proposed method. Compared to full fine-tuning, our approach achieves superior performance while requiring significantly fewer parameters, which uses only 2.71\% and 7.69\% of the parameters required for full fine-tuning in the object classification and the dense prediction respectively. The code will be released when received.}



Fine-tuning pre-trained models for point cloud analysis typically involves updating all model parameters, leading to substantial computational overhead. In this paper, we demonstrate that high performance can be achieved with a parameter-efficient approach that selectively updates only a minimal subset of parameters, focusing on local neighborhood information rather than exhaustive parameter updates and global attention mechanisms. Our method employs parameter-efficient transfer learning, optimizing task performance while minimizing parameter costs. Specifically, we freeze the parameters of conventional pre-trained models and introduce a Hierarchical Ladder Network (HLN) to directly capture essential local information from the input point cloud. To bridge this local information with the backbone’s intermediate global representations, we propose a Local-Global Fusion (LGF) module that integrates multi-scale features effectively. Additionally, our method generates dynamic prompts from these fused features to enhance the backbone’s capacity for producing optimized global features across diverse downstream tasks. Extensive experiments on point cloud classification and dense prediction tasks validate the effectiveness of our approach, which achieves superior performance with a fraction of the parameters required for full fine-tuning—using only 2.71\% and 7.69\% of the parameters for object classification and dense prediction, respectively. 

\end{abstract}