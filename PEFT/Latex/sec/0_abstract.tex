\begin{abstract}
\blk{Point cloud analysis has achieved outstanding performance by transferring point cloud pre-trained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient and ineffective. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pre-trained models and utilizes a hierarchical Ladder Network (HLN) to directly extract local information from the input point cloud. To aggregate this local information with the backbone network’s intermediate global activation, we introduce a Local-Global Fusion (LGF) module. Additionally, to further enhance performance, fused multi-scale features are used to generate dynamic prompts, enabling the backbone network to produce global features optimized for downstream tasks. A series of experiments on tasks such as point cloud object classification and dense prediction have demonstrated the effectiveness of our proposed method. Compared to full fine-tuning, our approach achieves superior performance while requiring significantly fewer parameters, which uses only 2.71\% and 7.69\% of the parameters required for full fine-tuning in the object classification and the dense prediction respectively. The code will be released when received.}


Point cloud analysis has advanced significantly with the use of pre-trained models, which traditionally rely on fine-tuning approaches that involve fully updating model parameters. 
However, in this study, we demonstrate that superior performance in point cloud analysis can be achieved by selectively updating a minimal subset of parameters and focusing on local neighborhood information, eliminating the need for exhaustive parameter updates and global attention mechanisms. 
Our approach leverages parameter-efficient transfer learning to optimize task performance while minimizing parameter overhead. 
Specifically, we freeze the parameters of standard pre-trained models and introduce a Hierarchical Ladder Network (HLN) to extract essential local information directly from the input point cloud. This focus on local information facilitates more refined feature extraction without excessive dependence on global context. 
To bridge this local information with the backbone network’s intermediate global representations, we propose a Local-Global Fusion (LGF) module, which effectively integrates multi-scale features. 
Furthermore, our approach employs dynamic prompts generated from these fused features, enhancing the backbone network’s ability to produce global features optimized for various downstream tasks. 
Comprehensive experiments across tasks (point cloud object classification, dense prediction), demonstrate the effectiveness of our method. 
Our approach achieves superior performance while using substantially fewer parameters compared to full fine-tuning, requiring only 2.71\% and 7.69\% of the parameters for object classification and dense prediction, respectively.

\end{abstract}