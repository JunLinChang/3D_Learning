\begin{abstract}
\blk{Point cloud analysis has achieved outstanding performance by transferring point cloud pre-trained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient and ineffective. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pre-trained models and utilizes a hierarchical Ladder Network (HLN) to directly extract local information from the input point cloud. To aggregate this local information with the backbone networkâ€™s intermediate global activation, we introduce a Local-Global Fusion (LGF) module. Additionally, to further enhance performance, fused multi-scale features are used to generate dynamic prompts, enabling the backbone network to produce global features optimized for downstream tasks. A series of experiments on tasks such as point cloud object classification and dense prediction have demonstrated the effectiveness of our proposed method. Compared to full fine-tuning, our approach achieves superior performance while requiring significantly fewer parameters, which uses only 2.71\% and 7.69\% of the parameters required for full fine-tuning in the object classification and the dense prediction respectively. The code will be released when received.}

\lr{
	Point cloud analysis has made significant strides through the use of pre-trained models via adapting model's input, output or intermediate layer, typically involve fully updating model parameters with fine-tuning paradigm. 
	In this paper, we show that parameter-efficient transfer learning can achieve superior performance with only updating a few of selective parameters or layers for point cloud analysis,
	aiming to balance task performance with parameter efficiency. 
	we retain the parameters of the standard pre-trained models and incorporate a Hierarchical Ladder Network (HLN) to extract local information directly from the input point cloud, which is important for dense prediction tasks.
	% (why local information)
	To integrate this local information with the backbone network's intermediate global activation, we propose a Local-Global Fusion (LGF) module. 
	Furthermore, to enhance performance, fused multi-scale features are employed to generate dynamic prompts, enabling the backbone network to produce global features optimized for downstream tasks. Extensive experiments on tasks such as point cloud object classification and dense prediction validate the effectiveness of our approach. Our method demonstrates superior performance with substantially fewer parameters compared to full fine-tuning, utilizing only 2.71\% and 7.69\% of the parameters required for full fine-tuning in object classification and dense prediction, respectively. }
\end{abstract}