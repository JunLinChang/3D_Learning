\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\appendix

\section{Dataset and Training Detail}

%All experiments are conducted on a single GeForce RTX 3090 GPU. During training, we freeze the weights of the pre-trained model and only fine-tune a small number of parameters for adding modules. The hyperparameter settings of the model are shown in Tab.~\ref{tab:paramas}.

All experiments were conducted using a single GeForce RTX 3090 GPU. During the training phase, the weights of the pre-trained model were kept fixed, and only a limited subset of parameters associated with the added modules were fine-tuned. The detailed hyperparameter configurations of the model are presented in Table~\ref{tab:paramas}


%\textbf{ScanObjectNN}~\cite{uy2019revisiting} comprises approximately 15k point cloud samples across 15 categories captured in indoor scenes, often containing background interference and occlusions. There are three commonly-used variants, namely OBJ\_BG, OBJ\_ONLY, and PB\_T50\_RS, with the difficulty escalating successively. The training settings are in line with the baseline comparison. Specifically, the AdamW optimizer is employed with an initial learning rate of $5e^{-4}$ and a weight decay of 0.05. A cosine learning rate scheduler with 10 epochs of warm-up is also utilized. The model undergoes a total of 300 epochs of training, with a batch size of 32. For the sake of consistency, our sampling point cloud contains 2048 points, which are divided into 128 groups of 32 points each within the Patch Encoder.

\textbf{ScanObjectNN}~\cite{uy2019revisiting} comprises approximately 15,000 point cloud samples distributed across 15 categories, collected from indoor scenes that frequently include background noise and occlusions. The dataset provides three commonly used variants: OBJ\_BG, OBJ\_ONLY, and PB\_T50\_RS, with progressively increasing levels of difficulty. The training configuration adheres to the settings used for baseline comparisons. Specifically, the AdamW optimizer is utilized with an initial learning rate of $5 \times 10^{-4}$ and a weight decay of 0.05. A cosine learning rate scheduler with a 10-epoch warm-up period is employed. The model is trained for a total of 300 epochs with a batch size of 32. To ensure consistency, each point cloud is sampled to 2048 points, which are partitioned into 128 groups, each containing 32 points, within the Patch Encoder.


%\textbf{ModelNet}~\cite{wu20153d} contains 12,311 clean 3D CAD models spanning 40 categories. Under the full-sample training setting, the network training configuration aligns with that used for ScanObjectNN. In the few-shot experiments, the model is trained for a total of 150 epochs. The input point cloud consists of 1,024 points, which are divided into 64 groups in the Patch Encoder, with each group containing 32 points.

\textbf{ModelNet}~\cite{wu20153d} consists of 12,311 clean 3D CAD models across 40 categories. The input point clouds are sampled to 1,024 points, which are organized into 64 groups within the Patch Encoder, with each group containing 32 points. Under the full-sample training configuration, the network settings are consistent with those used for ScanObjectNN. For the few-shot experiments, the model is trained for 150 epochs.


%\textbf{ShapeNetPart}~\cite{yi2016scalable} is a widely used benchmark for point-level synthetic object part segmentation, comprising 16,881 samples across 16 object categories and 50 part categories. Training is conducted with the AdamW optimizer, employing a weight decay of $1e^{-4}$ and a cosine learning rate scheduler. The initial learning rate is set to $2e^{-4}$, with 10 epochs of warm-up. Models are trained for 300 epochs with a batch size of 16. Each input consists of 2,048 points, organized into 128 groups, with each group containing 32 points.

\textbf{ShapeNetPart}~\cite{yi2016scalable} is a widely used benchmark for point-level synthetic object part segmentation, comprising 16,881 samples distributed across 16 object categories and 50 part categories. Training is performed using the AdamW optimizer with a weight decay of 0.05 and a cosine learning rate scheduler. The initial learning rate is set to $2 \times 10^{-4}$, with a warm-up period of 10 epochs. The models are trained for 300 epochs using a batch size of 16. Each input point cloud contains 2,048 points, which are grouped into 128 clusters, with each cluster containing 32 points.


%\textbf{S3DIS}~\cite{armeni20163d} is a large-scale indoor scene dataset encompassing six areas with a total of 273 million points across 13 categories. Consistent with prior studies~\cite{dong2022autoencoders}, we recommend using Area 5 for evaluation to ensure a more reliable and fair assessment of performance in semantic segmentation task. The model training employs a cosine learning rate scheduler, starting with an initial learning rate of $2e^{-4}$. Training is conducted over 60 epochs with a batch size of 32, while other configurations remain identical to those used for ShapeNetPart~\cite{yi2016scalable}.

\textbf{S3DIS}~\cite{armeni20163d} is a large-scale indoor scene dataset comprising six areas with a total of 273 million points annotated across 13 categories. Following established practices~\cite{dong2022autoencoders}, Area 5 is recommended for evaluation to provide a more reliable and standardized assessment of performance in semantic segmentation tasks. The model is trained using a cosine learning rate scheduler with an initial learning rate of $2 \times 10^{-4}$. Training is performed over 60 epochs with a batch size of 32, while other configurations are consistent with those used for ShapeNetPart~\cite{yi2016scalable}.


\section{Additianal Experiments}

%\subsection{Performance Analysis}
%As illustrated in Fig.~\ref{fig:performance}, we compared the speed and memory usage of our PLT with previous methods during both training and inference. From Fig.~\ref{fig:per1}, we observe that with smaller batch sizes, our training speed is initially slower than that of full fine-tuning and even lower than IDPT. However, as the batch size increases, the training speed of our method surpasses that of full fine-tuning. Specifically, with a batch size of 128, our method is nearly 30\% faster than IDPT. Fig.~\ref{fig:per2} shows that our method consistently uses less training memory than full fine-tuning. In contrast, both IDPT and Point PEFT exhibit higher memory usage than full fine-tuning. Notably, with a batch size of 256, our approach reduces memory usage by 1.2 GB compared to full fine-tuning. As depicted in Fig.~\ref{fig:per3}, our method consistently outperforms IDPT and Point PEFT in inference speed, coming close to but slightly below that of full fine-tuning. Furthermore, as shown in Fig.~\ref{fig:per4}, the difference in memory usage during inference between our method and full fine-tuning is minimal. In comparison, IDPT and Point PEFT exhibit significantly higher memory usage differences, particularly at a batch size of 512.

\subsection{Performance Analysis}  
As shown in Fig.~\ref{fig:performance}, we evaluated the speed and memory usage of our proposed PLT approach in comparison with previous methods during both training and inference. In Fig.~\ref{fig:per1}, it can be observed that, with smaller batch sizes, the training speed of our method is initially slower than that of full fine-tuning and even falls below that of IDPT~\cite{zha2023instance}. However, as the batch size increases, the training speed of our approach surpasses that of full fine-tuning. Specifically, at a batch size of 128, our method achieves a training speed nearly 30\% faster than IDPT~\cite{zha2023instance}. Fig.~\ref{fig:per2} demonstrates that our method consistently requires less memory for training compared to full fine-tuning. By contrast, both IDPT~\cite{zha2023instance} and Point-PEFT~\cite{tang2024point} exhibit greater memory usage than full fine-tuning. Notably, at a batch size of 256, our approach reduces memory consumption by 1.2 GB relative to full fine-tuning. Regarding inference speed, as depicted in Fig.~\ref{fig:per3}, our method consistently outperforms IDPT~\cite{zha2023instance} and Point-PEFT~\cite{tang2024point}, achieving performance close to, albeit slightly below, that of full fine-tuning. Additionally, Fig.~\ref{fig:per4} indicates that the difference in memory usage during inference between our method and full fine-tuning is minimal. Conversely, IDPT~\cite{zha2023instance} and Point-PEFT~\cite{tang2024point} exhibit substantially higher memory usage, particularly at a batch size of 512.


%\subsection{Attention Score of LGF}
%Fig.~\ref{fig:attention_score} shows that, on the most challenging version (i.e., PB\_T50\_RS) ) of ScanObjectNN, the local attention scores (LGFs) across all categoris and layers are consistently below 0.5. This highlights the critical role of pretrained global prior information in downstream task learning. Additionally, the attention scores vary across categories, with noticeable differences in shallow layers and more uniformity in deeper layers. This suggests that the fusion of local and global information exhibits high specificity in the shallow layers for different categories. Moreover, the attention scores of local information tend to decrease progressively from shallow to deeper layers in most categories, indicating that the network emphasizes learning local information in the early layers while prioritizing global information in the later stages.

\subsection{Attention Score of LGF}  
As illustrated in Fig.~\ref{fig:attention_score}, the local attention scores of LGF across all categories and layers remain consistently below 0.5 on the most challenging variant of ScanObjectNN~\cite{uy2019revisiting} (i.e., PB\_T50\_RS). This underscores the pivotal role of pretrained global prior information in facilitating downstream task learning. Furthermore, the attention scores exhibit category-dependent variations, with more pronounced differences in shallow layers and greater uniformity in deeper layers. This observation suggests that the integration of local and global information is highly specific to different categories in the shallow layers, while becoming more generalized in the deeper layers. Additionally, the attention scores for local information tend to decline progressively from shallow to deeper layers across most categories. This trend indicates that the network prioritizes learning local information in the early layers, while progressively shifting its focus toward global information in the later stages.


%\subsection{Token Selections for Head Inputs}
%We analyzed the impact of different input tokens on downstream task heads, including the pooling of class token, prompt tokens, point patch tokens, and HLN output tokens. As illustrated in Figure 3, we evaluated five standard token selection methods. The results show that the best performance is achieved when all four token types are included. Interestingly, adding the pooled prompt tokens to the class token, rather than concatenating them, improves accuracy by 0.55\% while reducing the number of trainable parameters. Additionally, we observed that removing the point patch tokens leads to a more significant decline in performance compared to removing the prompt tokens or HLN output tokens. This highlights the critical role of prior information from pre-trained models in achieving high-quality downstream task performance.

\subsection{Token Selections for Head Inputs}  
We investigated the influence of various input tokens on the downstream task heads, including class token, the pooling of prompt tokens, the pooling of point patch tokens, and the pooling of HLN output tokens. As shown in Fig.~\ref{fig:head}, we evaluated five standard token selection strategies. The results indicate that the highest performance is achieved when all four token types are utilized. Notably, adding the pooling of prompt tokens to the class token, rather than concatenating them, improves accuracy by 0.55\% while reducing the number of trainable parameters. Furthermore, we observed that removing the point patch tokens results in a more substantial decline in performance compared to the removal of either the prompt tokens or HLN output tokens. This underscores the essential role of prior information from pre-trained models in enabling high-quality performance on downstream tasks.

\subsection{Qualitative Analysis}

%\subsubsection{t-SNE Visualizations}
%Fig.~\ref{fig:tsne} visualizes the t-SNE feature manifolds generated by fully fine-tuned methods, point-cloud-specific fine-tuning approaches such as IDPT and DAPT, and our proposed PLT, all trained on the ScanObjectNN PB\_T50\_RS dataset. The dispersion of points between categories in the t-SNE plot reflects the quality of the model's feature representation—greater inter-cluster dispersion and smaller intra-cluster average  distance  indicates stronger discriminative power, making classification tasks easier.

%Notably, our PLT exhibits significantly greater inter-cluster dispersion alongside a smaller intra-cluster average  distance compared to previous methods. This suggests that our method effectively enhances the separation of features between categories while maintaining compactness within each category. Such properties indicate a superior ability to learn and represent discriminative features.

%Moreover, PLT achieves these results while utilizing fewer learnable parameters, demonstrating its efficiency in leveraging pre-trained models to adapt to downstream tasks. This advantage not only reduces the computational cost but also highlights PLT’s potential for broader applicability in resource-constrained scenarios. Compared to fully fine-tuned and point cloud-specific methods, PLT strikes a better balance between parameter efficiency and representation quality, establishing its value in point-cloud-based learning tasks.

\subsubsection{t-SNE Visualizations}  
Fig.~\ref{fig:tsne} presents the t-SNE~\cite{van2008visualizing} visualizations of feature manifolds generated by fully fine-tuned methods, point-cloud-specific fine-tuning approaches such as IDPT~\cite{zha2023instance} and DAPT~\cite{zhou2024dynamic}, and our proposed PLT, all trained on the ScanObjectNN PB\_T50\_RS dataset~\cite{uy2019revisiting}. The dispersion of points between categories in the t-SNE plot reflects the quality of the model's feature representation—greater inter-cluster dispersion and smaller intra-cluster average distance indicate stronger discriminative power, facilitating easier classification.

Notably, our PLT demonstrates significantly greater inter-cluster dispersion and a smaller intra-cluster average distance compared to previous methods. This suggests that our approach effectively enhances feature separation between categories while maintaining compactness within each category, indicative of a superior ability to learn and represent discriminative features.

Furthermore, PLT achieves these results while using fewer learnable parameters, showcasing its efficiency in leveraging pre-trained models for adaptation to downstream tasks. This not only reduces computational cost but also highlights PLT's potential for broader applicability in resource-constrained environments. In comparison to fully fine-tuned and point-cloud-specific methods, PLT strikes an optimal balance between parameter efficiency and representation quality, underscoring its value in point cloud learning tasks.

\input{fig/supplement/attention_score}

%\subsubsection{Part Segmentation Visualizations}

%In Fig.~\ref{fig:part_segmentation}, we present a visual demonstration of the partial segmentation outcomes achieved by our proposed PLT, which is based on the PointMAE baseline. Specifically, we choose five representative samples from different categories. For each of these selected samples, we render three distinct perspectives. It is evident from the figure that our method demonstrates remarkable segmentation performance. Notably, our PLT method achieves high accuracy and precisely distinguishes object parts while maintaining efficiency with a minimal parameter set.

%\subsubsection{Semantic Segmentation Visualizations}

%Fig.~\ref{fig:semantic} presents a qualitative comparison of segmentation results between our proposed PLT method and previous fine-tuning approaches on semantic segmentation tasks in Area 5 of the S3DIS dataset. The comparison highlights the strengths of PLT in addressing common challenges associated with point cloud segmentation. From the first row of the figure, it is evident that full fine-tuning, despite being resource-intensive, struggles with accurately segmenting object boundaries. This limitation often leads to poor boundary delineation and misclassification in continuous regions, largely due to inadequate capture of point cloud local information. A more detailed comparison in office\_9 demonstrates that methods such as full fine-tuning, IDPT, and DAPT misclassify walls as beams and clutter areas as ceilings. In contrast, our PLT approach successfully addresses these issues, accurately segmenting clutter regions while significantly reducing wall misclassification. Similarly, in office\_35, PLT outperforms other methods in segmenting challenging areas such as the clutter region and the board. Where methods like full fine-tuning, IDPT, and PPT struggle to recognize or segment these regions effectively, PLT demonstrates superior performance, maintaining clarity and precision. These results underscore the capability of our PLT method to achieve outstanding segmentation performance while using significantly fewer fine-tuning parameters. Compared to other approaches, PLT not only produces clearer segmentation boundaries but also exhibits a marked reduction in misclassifications, confirming its efficiency and effectiveness for point cloud semantic segmentation tasks.

\subsubsection{Part Segmentation Visualizations}  
Fig.~\ref{fig:part_segmentation} provides a visual demonstration of the part segmentation results obtained using our proposed PLT, based on the PointMAE~\cite{pang2022masked} baseline. We select five representative samples from various categories and render three distinct perspectives for each sample. The figure clearly illustrates that our method delivers exceptional segmentation performance. Notably, the PLT method achieves high accuracy in distinguishing object parts with remarkable precision, while maintaining efficiency by using a minimal set of parameters.

\subsubsection{Semantic Segmentation Visualizations}  
Fig.~\ref{fig:semantic} presents a qualitative comparison of segmentation results between our proposed PLT method and prior fine-tuning approaches in semantic segmentation tasks on Area 5 of the S3DIS dataset~\cite{armeni20163d}. The comparison highlights the advantages of PLT in addressing common challenges in point cloud segmentation. From the first row of the figure, it is evident that while full fine-tuning is resource-intensive, it struggles to accurately segment object boundaries. This often results in poor boundary delineation and misclassification in continuous regions, primarily due to insufficient capture of local point cloud information. A more detailed comparison in the office\_9 scene reveals that methods such as full fine-tuning, IDPT~\cite{zha2023instance}, and DAPT~\cite{zhou2024dynamic} incorrectly classify walls as beams and cluttered regions as ceilings. In contrast, our PLT method successfully segments clutter regions while significantly reducing the misclassification of walls. Similarly, in the office\_35 scene, PLT outperforms other methods in segmenting challenging areas such as the cluttered region and the board. Whereas methods like full fine-tuning, IDPT~\cite{zha2023instance}, and PPT~\cite{zhang2024positional} fail to accurately recognize or segment these regions, PLT demonstrates superior performance, maintaining clarity and precision. These results emphasize the ability of the PLT method to deliver outstanding segmentation performance while using far fewer fine-tuning parameters. Compared to other approaches, PLT not only provides clearer segmentation boundaries but also significantly reduces misclassifications, further validating its efficiency and effectiveness for point cloud semantic segmentation tasks.


\input{tab/appendix/parames}

\input{fig/tsne}

\input{fig/head}
\input{fig/supplement/performance}

\input{fig/supplement/part_segmentation}

\input{fig/supplement/semantic_segmentation}
