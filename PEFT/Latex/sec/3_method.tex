% \section{Preliminary}
% \label{sec:preliminary}

% \textbf{Ladder Side Tuning}\cite{sung2022lst} trains a ladder side network, a small and separate network that takes intermediate activations from the backbone transformer as input and makes predictions. It includes a downward projection $h_i^f=x_iW_d^T$ to decrease dimension of the feature and a gated connection to integrate the intermediate activation of the backbone $f$ and the features of the side network $g$ Adaptively. The output is calculated by
% \begin{equation}
%     \boldsymbol{h}_{i}^{g}=\Phi(\mu_{i} * \boldsymbol{h}_{i}^{f}+\left(1-\mu_{i}\right) * \boldsymbol{h}_{i-1}^{g})
% \end{equation}
% where $\mu_{i}=\operatorname{sigmoid}\left(\frac{\alpha_{i}}{T}\right)$ is a gate parameterized with a learnable zero-initialized scalar $\alpha_i$ and temperature $T$(= 0.1), $\Phi$ is a linear layer that projects the fused features.

% \textbf{Prompt tuning}\cite{li2021prefix,jia2022visual} freezes the backbone network during fine-tuning while inserting learnable tokens into the input token sequence. Prompt tokens interact with the original tokens through attention mechanisms. Given the class token $x_{cls}\in R^{1 
% \times d}$ and patch tokens $T \in R^{N \times d}$, where $N$ is the num of patch tokens, we can insert prompt tokens $P_i \in R^{n \times d}$ for each layer following VPT-Deep\cite{jia2022visual}, where $n \ll N$ is the num of prompt tokes.The output of $i$-th layer($L(\cdot)$) is expressed as follows:
% \begin{equation}
%     \boldsymbol{x}_{i}=L_{i}\left(\left[\boldsymbol{T}_{c l s} ; \boldsymbol{P}_{i} ; \boldsymbol{T}\right]\right)
% \end{equation}

\input{fig/overall}

\section{Methodology}
\label{sec:methodology}
%Fine-tuning a pre-trained model has achieved impressive results, but it requires a significant training cost.To address this, We combine ladder side tuning with prompt tuning to propose a novel point cloud fine-tuning method called Point Ladder Tuning (PLT), which achieves a better balance between performance and parameter efficiency for 3d point cloud learning. The overall framework is illustrated in Fig..In our method, we introduce a hierarchical ladder network based on LST to extract local information from the raw input and fuse it with the global intermediate activations of the backbone network through a local-global fusion module(LGF). To further enhance performance, we also generate prompt tokens derived from the fused features, which are used to fine-tune the global features of the backbone network.



%Fine-tuning pre-trained models has demonstrated remarkable success across various domains, yet it often comes with high computational and storage costs. To address this issue, we propose Point Ladder Tuning (PLT), a novel fine-tuning approach that combines the strengths of Ladder Side Tuning (LST) and prompt tuning to achieve a more efficient balance between performance and parameter usage for 3D point cloud learning. 


%LST primarily focuses on statically weighting and fusing the global intermediate activations from each layer, neglecting the equally critical local information essential for downstream tasks. This limitation leads to suboptimal performance, particularly in point cloud neighborhoods where local features play a crucial role. On the other hand, Prompt Tuning suffers from the issue of randomly initializing the Prompt Token, which makes optimization significantly more challenging. Additionally, once trained, the Prompt Token remains fixed across all instances, lacking the adaptability needed to capture instance-specific variations. This lack of adaptability further contributes to degraded performance, as the model fails to effectively adjust to the diverse characteristics present in different data instances.

%In contrast to these methods, our PLT addresses these limitations by incorporating both global and local features effectively, ensuring better optimization and adaptability to individual instances, which ultimately leads to improved performance in complex tasks like point cloud semantic segmentation.

%At the core of PLT, we introduce a hierarchical Ladder Network based on LST, which extracts local features directly from the raw input point cloud. These local features are then integrated with the global intermediate activations of the pre-trained backbone network through a Local-Global Fusion (LGF) module, facilitating the creation of multi-scale representations. To further improve performance, we generate prompt tokens from the fused local-global features, which are then used to fine-tune the global features of the backbone network. This approach significantly reduces the number of parameters that need to be updated, while retaining the rich semantic knowledge acquired during pre-training, thereby enhancing task-specific performance across various point cloud applications. Through this method, we offer a more computationally efficient solution without compromising the effectiveness of the pre-trained model.


Fine-tuning pre-trained models, though effective across domains, often incurs high computational and storage costs. To address this, we propose Point Ladder Tuning (PLT), an efficient fine-tuning approach for 3D point cloud learning that combines Ladder Side Tuning (LST) and prompt tuning to optimize parameter efficiency and task performance. While LST statically weights global activations but overlooks crucial local features, and prompt tuning lacks adaptability across instances, PLT integrates both local and global features to enhance optimization and adaptability.

PLT introduces a hierarchical Ladder network (HLN) that directly extracts local features from the input point cloud and fuses them with the global activations of the backbone network to create multi-scale representations. The HLN consists of two main components: (1) a set abstraction module, detailed in Sec.~\ref{sec:HLN}, and (2) a Local-Global Fusion module (LGF), discussed in Sec.~\ref{sec:LGF}. Further, dynamic prompt tokens generated from these fused features refine the backboneâ€™s global features, achieving high task-specific performance with minimal parameter updates, which is described in Sec.~\ref{sec:FT_backbone}. This approach thus maintains the semantic richness of pre-trained models while substantially reducing computational demands, demonstrating effectiveness across point cloud applications like semantic segmentation.
Our framework is shown in Fig.~\ref{fig:overall2}.


%\subsection{Hierarchical Ladder Network}
%\label{sec:HLN}
%Pre-trained models typically learn the global information of input point clouds through attention mechanisms but often overlook the importance of locality, which is crucial for downstream tasks, especially dense prediction tasks. Although Point PEF introduces a Geometry-Aware Adapter to capture local information, it still relies on the global intermediate activations of pre-trained models, which limits its ability to effectively learn local features within the original point cloud. Additionally, because pre-trained models often operate at a single scale throughout the learning process, lacking multi-scale information, this constraint further impedes performance on downstream tasks.

%To address these problems, we introduce a hierarchical Ladder network to better capture the intrinsic local information of the input point cloud. The module includes multiple HLN layers that are capable of capturing point cloud features at different scales, which has been shown to be effective in subsequent experiments. The HLN Layer consists of two main components: (1) Set Abstraction (SA), and (2) Local-Global Fusion Module (LGF). Firstly, we will introduce the SA in this section and the LGF in Sec.~\ref{sec:LGF}.

%Given the input point cloud $P \in R^{N \times 3}$, where N is the num of the input point cloud, we first apply Point Embedding to map it into a high-dimensional space, obtaining the point cloud feature $F \in R^{N \times C}$, where is the dimension of the point cloud feature. Next, we use Set Abstraction (SA) to perform downsampling. This process begins with farthest point sampling (FPS) to select a set of center points $C=\{c_1, \ldots, c_n\}$. Then, we use k-nearest neighbors (KNN) to construct local neighborhoods $P(c)=\{p^1_c, \ldots,  p^k_c\}$, $F(c)=\{f^1_c, \ldots, f^k_c\}$ for each center point $c$. Finally, we apply max pooling followed by a multi-layer perceptron (MLP) to obtain the output point cloud. The formula for this process is as follows:
%\begin{flalign}
%    \boldsymbol{f}_{c} & = \varphi \left(\left[\boldsymbol{f}_{c}^{j} ;\left(\boldsymbol{p}_{c}-\boldsymbol{p}_{c}^{j}\right)\right]\right)\\
%    \boldsymbol{f}_{c}^{'} & = \boldsymbol{f}_{c} + \rho \left( h_{\boldsymbol{\Theta}}\left ( \boldsymbol{f}_{c} \right) \right)
%\end{flalign}
%where $\rho$ and $\varphi$ represent a MLP respectively, $[\cdot, \cdot]$ represents concat operation. And $h_{\boldsymbol{\Theta}}$ represents the aggregation function. Unless otherwise specified, we use Max-Pooling.


\subsection{Hierarchical Ladder Network}
\label{sec:HLN}
Pre-trained models primarily capture global information from input point clouds through attention mechanisms; however, they often overlook local features, which are essential for downstream tasks, particularly dense prediction. Although PointPEF introduces a Geometry-Aware Adapter to enhance local feature learning, it still relies heavily on the global intermediate activations of pre-trained models, limiting its ability to effectively capture intrinsic local features from the original point cloud. Furthermore, since most pre-trained models operate at a single scale, they lack the multi-scale information necessary for optimal performance in downstream tasks.

To address these limitations, we propose a Hierarchical Ladder Network (HLN) to capture local information across multiple scales within the input point cloud. The HLN module includes multiple layers designed to capture features at varying scales, and our experiments validate its effectiveness. The HLN layer consists of two key components: (1) Set Abstraction (SA) and (2) Local-Global Fusion Module (LGF). 
In this section, we introduce SA, with LGF discussed further in Sec.~\ref{sec:LGF}.

Given an input point cloud $P \in \mathbb{R}^{N \times 3}$, where $N$ is the number of points, we first apply point embedding to map $P$ into a high-dimensional space, resulting in a point cloud feature matrix $F \in \mathbb{R}^{N \times C}$, where $C$ represents the feature dimension. We then use Set Abstraction (SA) for downsampling, starting with farthest point sampling (FPS) to select a set of center points $C=\{c_1, \ldots, c_n\}$. Next, k-nearest neighbors (KNN) constructs local neighborhoods $P(c)=\{p^1_c, \ldots, p^k_c\}$ and $F(c)=\{f^1_c, \ldots, f^k_c\}$ for each center point $c$. Finally, max pooling and a multi-layer perceptron (MLP) produce the output point cloud features, as defined by the following equations:
\begin{flalign}
	\boldsymbol{f}_{c} & = \varphi \left(\left[\boldsymbol{f}_{c}^{j} ;\left(\boldsymbol{p}_{c}-\boldsymbol{p}_{c}^{j}\right)\right]\right)\\
	\boldsymbol{f}_{c}^{'} & = \boldsymbol{f}_{c} + \rho \left( h_{\boldsymbol{\Theta}}\left ( \boldsymbol{f}_{c} \right) \right)
\end{flalign}
where $\rho$ and $\varphi$ denote MLPs, $[\cdot, \cdot]$ indicates the concatenation operation, and $h_{\boldsymbol{\Theta}}$ represents the aggregation function. Unless otherwise specified, max pooling is used.





%\subsection{Local-Global Fusion Module}
%\label{sec:LGF}
%To make better use of the local information captured in the PLT Branch and the global information captured in the pre-trained backbone network, we propose a local-global fusion module that adaptively aggregates both types of information through selective attention. Given the input point cloud $P_{s}$ and their corresponding features $F_{s}$ from the HLN layer, as well as the output point cloud $P_{m}$ and its corresponding features $F_{m}$ from the backbone network, we firstly use $W$ to map the output features of the backbone network to match the same feature dimension as the input point cloud feature vectors of the HLN layer:
%\begin{equation}
%    \boldsymbol{F}_{m}^{'} = \boldsymbol{F}_mW 
%\end{equation}
%
%Next, we use the same operations as Set Abstraction (SA) to extract both the global features $F_g$, which are derived from the interaction between the input point cloud $P_s$ of the HLN layer and the output point cloud $P_m$ of the backbone network, and the local features $F_l$, which are obtained from the interaction between the input point clouds $P_s$ of the HLN layer.
%
%Finally, we apply a selective attention mechanism to adaptively fuse the global and local features. This mechanism allows the model to focus on the most relevant information from each source, ensuring that both global context and fine-grained local details are effectively integrated. Firstly, we apply the aggregation function $\mathcal{A}$, with average pooling as the default, to the global and local features to obtain the corresponding global and local feature vectors:
%\begin{equation}
%    \boldsymbol{f}_g, \boldsymbol{f}_l = \mathcal{A}(\boldsymbol{F}_g),  \mathcal{A}(\boldsymbol{F}_l)
%\end{equation}
%
%Then, we add the global and local features and pass them through a MLP to obtain $z_g$ and $z_l$:
%\begin{equation}
%    \boldsymbol{z}_g, \boldsymbol{z}_l = \alpha \left( \boldsymbol{f}_g + \boldsymbol{f}_l \right)
%\end{equation}
%where $\alpha$ represents a MLP.
%
%Next, we apply Softmax to compute the attention weights $s_g$ and $s_l$ for the global and local features:
%\begin{equation}
%    \boldsymbol{s}_g^i = \frac{e^{\boldsymbol{z}_g^i}}{e^{\boldsymbol{z}_g^i} + e^{\boldsymbol{z}_l^i}}, \boldsymbol{s}_l^i = \frac{e^{\boldsymbol{z}_l^i}}{e^{\boldsymbol{z}_g^i} + e^{\boldsymbol{z}_l^i}}
%\end{equation}
%
%Subsequently, the global and local features are weighted according to their attention scores to produce the fused multi-scale features:
%\begin{equation}
%    \boldsymbol{F}_{ms} = \boldsymbol{s}_g \boldsymbol{F}_g + \boldsymbol{s}_l \boldsymbol{F}_l
%\end{equation}
%
%Finally, we use an MLP $\tau$ to perform feature mapping and add the result to the input features:
%\begin{equation}
%     \boldsymbol{F}_{o} = \boldsymbol{F}_s + \tau \left( \boldsymbol{F}_{ms} \right)
%\end{equation}
%
%By selectively emphasizing the most relevant local and global features through attention, our LGF ensures the effective integration of both types of information, enhancing the model's ability to capture multi-scale features. This is crucial for tasks that require a balance between fine-grained local details and the broader global structure of point clouds, especially in dense prediction tasks.


\subsection{Local-Global Fusion Module}
\label{sec:LGF}
To effectively leverage the local features captured in the PLT branch alongside the global features from the pre-trained backbone, we introduce a Local-Global Fusion (LGF) module that adaptively combines both types of information via selective attention. Given an input point cloud $P_{s}$ with corresponding features $F_{s}$ from the HLN layer, and an output point cloud $P_{m}$ with features $F_{m}$ from the backbone network, we first map the backbone features to match the dimensionality of the HLN layerâ€™s feature vectors using a learnable matrix $W$:
\begin{equation}
	\boldsymbol{F}_{m}^{'} = \boldsymbol{F}_mW 
\end{equation}

We then perform Set Abstraction (SA) operations to extract global features $F_g$, which represent interactions between $P_s$ and $P_m$, and local features $F_l$, derived from the interactions within $P_s$ in the HLN layer.

To fuse these global and local features, we employ a selective attention mechanism. This mechanism allows the model to focus adaptively on the most relevant information from each feature source, ensuring both global context and local details are integrated. First, we apply an aggregation function $\mathcal{A}$, typically average pooling, to obtain global and local feature vectors:
\begin{equation}
	\boldsymbol{f}_g, \boldsymbol{f}_l = \mathcal{A}(\boldsymbol{F}_g),  \mathcal{A}(\boldsymbol{F}_l)
\end{equation}

These global and local features are then summed and passed through an MLP to produce representations $z_g$ and $z_l$:
\begin{equation}
	\boldsymbol{z}_g, \boldsymbol{z}_l = \alpha \left( \boldsymbol{f}_g + \boldsymbol{f}_l \right)
\end{equation}
where $\alpha$ represents an MLP.

Using Softmax, we compute attention weights $s_g$ and $s_l$ for the global and local features:
\begin{equation}
	\boldsymbol{s}_g^i = \frac{e^{\boldsymbol{z}_g^i}}{e^{\boldsymbol{z}_g^i} + e^{\boldsymbol{z}_l^i}}, \quad \boldsymbol{s}_l^i = \frac{e^{\boldsymbol{z}_l^i}}{e^{\boldsymbol{z}_g^i} + e^{\boldsymbol{z}_l^i}}
\end{equation}

The fused multi-scale features are then computed as the weighted sum of the global and local features:
\begin{equation}
	\boldsymbol{F}_{ms} = \boldsymbol{s}_g \boldsymbol{F}_g + \boldsymbol{s}_l \boldsymbol{F}_l
\end{equation}

Finally, an MLP $\tau$ performs feature mapping, and the output is added to the input features to produce the final fused output:
\begin{equation}
	\boldsymbol{F}_{o} = \boldsymbol{F}_s + \tau \left( \boldsymbol{F}_{ms} \right)
\end{equation}

By adaptively emphasizing the most relevant local and global features, our LGF module effectively integrates multi-scale information, enhancing the model's capacity to capture both fine-grained details and broad structural context in point cloudsâ€”particularly valuable for dense prediction tasks.


%\subsection{Fine-tuning on the Backbone Network}
%\label{sec:FT_backbone}
%To enable the pre-trained backbone network to generate more effective global features for downstream tasks, we propose a dynamic prompt generation method based on the multi-scale point cloud features output $F_{o}$ by the Local-Global Fusion (LGF) module. Firstly, $F_{o}$ is mean-pooled to obtain a compact representation. These pooled features are then scaled and translated using learnable parameters $\gamma$ and $\beta$ to generate multi-scale prompt $p$. To efficiently utilize parameters, we reuse $W$ to align the dimensions of the multi-scale prompts with the dimensions of the backbone network features. The formula can be expressed as follows:
%\begin{equation}
%    p = \left( \boldsymbol{\gamma} \times \frac{1}{n}\sum_i^n{\boldsymbol{F}_{o}^i} + \boldsymbol{\beta} \right) W^T
%\end{equation}
%where n is the num of tokens in $F_o$.
%
%Finally, we incorporate multi-scale prompts into the backbone network for learning. The output of the $l$-th transformer layer $x_l$ can be expressed as
%\begin{equation}
%    \boldsymbol{x}_l=L_l\left(\left[\boldsymbol{T}_{c l s} ; \boldsymbol{p}_{0}, \ldots, \boldsymbol{p}_{i-1} ; \boldsymbol{T}\right]\right)
%\end{equation}
%
%To enhance performance further, we follows ssf~\cite{lian2022scaling} and applies scaling and shifting to the output of each module within the backbone network, enabling the extraction of task-specific global information. Given an input $x$, the output $y$ can be expressed as:
%\begin{equation}
%    \boldsymbol{y} = \boldsymbol{s} \times \boldsymbol{x} + \boldsymbol{t}
%\end{equation}
%where s and t represents scaling and shifting parameter respectively, which are learnable.


\subsection{Fine-tuning on the Backbone Network}
\label{sec:FT_backbone}
To enable the pre-trained backbone network to produce enhanced global features tailored for downstream tasks, we propose a dynamic prompt generation method based on the multi-scale point cloud feature output $F_{o}$ from the Local-Global Fusion (LGF) module. First, $F_{o}$ is mean-pooled to obtain a compact representation. This pooled feature is then scaled and shifted using learnable parameters $\gamma$ and $\beta$, generating a multi-scale prompt $p$. To maximize parameter efficiency, we reuse $W$ to align the dimensionality of the multi-scale prompts with that of the backbone network features. The formula for this process is as follows:
\begin{equation}
	p = \left( \boldsymbol{\gamma} \times \frac{1}{n}\sum_i^n{\boldsymbol{F}_{o}^i} + \boldsymbol{\beta} \right) W^T
\end{equation}
where $n$ is the number of tokens in $F_o$.

Finally, the multi-scale prompts are incorporated into the backbone network during training. The output of the $l$-th transformer layer, $x_l$, is given by:
\begin{equation}
	\boldsymbol{x}_l = L_l\left(\left[\boldsymbol{T}_{cls} ; \boldsymbol{p}_{0}, \ldots, \boldsymbol{p}_{i-1} ; \boldsymbol{T}\right]\right)
\end{equation}

To further enhance performance, we follow SSF~\cite{lian2022scaling} and apply scaling and shifting to the output of each module within the backbone network, enabling the extraction of task-specific global information. Given an input $x$, the output $y$ is calculated as:
\begin{equation}
	\boldsymbol{y} = \boldsymbol{s} \times \boldsymbol{x} + \boldsymbol{t}
\end{equation}
where $s$ and $t$ are learnable scaling and shifting parameters, respectively.


