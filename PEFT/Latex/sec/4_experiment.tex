\input{tab/sota}

\section{Experiments}
\label{sec:experiments}
In this section, we conduct an extensive evaluation of the proposed Point Ladder Tuning (PLT) method across multiple point cloud datasets and tasks. Our experiments aim to validate the effectiveness of PLT in achieving high performance while maintaining parameter efficiency, comparing it against state-of-the-art methods to highlight its advantages.
\subsection{Experimental Settings}
To ensure a fair comparison, we adopted the same experimental setup as used in previous fine-tuning methods~\cite{zha2023instance, zhou2024dynamic} for each baseline. During training, the weights of the pre-trained model were frozen, and only a limited number of parameters in the added modules were fine-tuned. All experiments were performed on a single GeForce RTX 3090 GPU.

\subsection{3D Object Classification}
\label{sec:classification}
\textbf{Object Classification on Real-World Dataset.} While most pre-trained point cloud models are trained on synthetic datasets like ShapeNet~\cite{chang2015shapenet}, which consist of clean, uniformly distributed point clouds, real-world point clouds often present additional challenges such as noise, missing data, and diverse distributions. To evaluate performance in these more realistic scenarios, we use the ScanObjectNN dataset~\cite{uy2019revisiting}, which comprises approximately 15k point cloud samples across 15 categories captured in indoor scenes, often containing background interference and occlusions. As shown in Tab.~\ref{tab:sota}, we evaluate PLT on three variants of the ScanObjectNN dataset (OBJ\_BG, OBJ\_ONLY, and PB\_T50\_RS), using PointBERT~\cite{yu2022point} and PointMAE~\cite{pang2022masked} as baselines.

Our PLT method demonstrates notable accuracy improvements over full fine-tuning in all settings, using only 2.71\% of the parameters. Specifically, PLT achieves gains of 4.14\%, 1.73\%, and 3.02\% on the three ScanObjectNN variants with PointBERT. Compared to the state-of-the-art model, PointGST, PLT achieves accuracy improvements of 0.45\% on PointBERT and 0.17\% on PointMAE under the most challenging PB\_T50\_RS setting in ScanObjectNN, showcasing the robustness and efficiency of our approach in handling real-world point cloud data.


%\textbf{Object Classification on Real-World Dataset.} Pre-trained Point cloud models are typically trained on the ShapeNet dataset~\cite{chang2015shapenet}, which consists of clean, uniformly distributed point clouds. However, real-world point clouds often suffer from issues such as noise and missing point, leading to diverse and challenging distributions. To evaluate the performance in these more realistic conditions, we use the ScanObjectNN dataset~\cite{uy2019revisiting}, which contains approximately 15k point cloud samples across 15 categories. These point clouds are captured in indoor scenes and often include background interference and occlusions from other objects. As shown in Tab.\ref{tab:sota} We conduct experiments on on three variants of the ScanObjectNN dataset~\cite{uy2019revisiting} (OBJ\_BG, OBJ\_ONLY, and PB\_T50\_RS), using two baseline models, PointBERT~\cite{yu2022point} and PointMAE~\cite{pang2022masked}, to assess the effectiveness of our PLT. 

%Notably, PLT achieved better accuracy than full fine-tuning across all settings while utilizing only 2.71\% of the parameters. Especially, PLT achieved increases of 4.14\%, 1.73\%, and 3.02\% in the three ScanObjectNN~\cite{uy2019revisiting} variants on Point-BERT. Compared to the state-of-the-art model PointGST, our LST improved accuracy by 0.45\% on PointBERT~\cite{yu2022point} and 0.17\% on PointMAE~\cite{pang2022masked} under the most challenging setting, PB\_T50\_RS, in ScanObjectNN~\cite{uy2019revisiting}.

%\textbf{Object Classification on Synthetic Dataset.} We also conduct experiments on the ModelNet dataset~\cite{wu20153d}, which includes 12,311 clean 3D CAD models across 40 categories. Following DAPT~\cite{zhou2024dynamic}, we split the ModelNet40 dataset into 9,843 training samples and 2,468 testing samples. During training, we applied standard data augmentation techniques, including random scaling and random translation. As shown in Tab.\ref{tab:sota}, without voting, our PLT achieved accuracy rates of 93.8\% and 93.5\% on PointMAE and PointBERT, respectively, which were 0.6\% and 0.8\% higher than full fine-tuning.. With voting, PLT continues to outperform full fine-tuning, especially on PointBERT, with an accuracy increase of 1.0\%.

%\textbf{Few-shot Learning.}We further conduct experiments on ModelNet40 to evaluate its transfer learning ability in few-shot setting. Following prior work~\cite{pang2022masked,zha2023instance,zhou2024dynamic}, we adopt the n-way, n-shot setting. As shown in Tab.~\ref{tab:fewshot}, PLT outperforms fully fine-tuned models and state-of-the-art models like IDPT~\cite{zha2023instance} and DAPT~\cite{zhou2024dynamic} in most cases, demonstrating its effectiveness in few-shot learning.

%\textbf{Compared with Other PETL Methods.}As illustrated in Tab.~\ref{tab:origin_finetuning}, using VPT~\cite{jia2022visual} for fine-tuning in PointMAE~\cite{pang2022masked} leads to a notable performance drop compared to full fine-tuning, particularly with a 4.09\% decrease in the PB\_T5\_RS setting. Similarly, while adapters~\cite{houlsby2019parameter} shows slight improvements on the OBJ\_ONLY task, but significant performance drops in other settings. LST~\cite{sung2022lst} demonstrate substantial gains on OBJ\_ONLY, but underperformed elsewhere. We also compare PEFT methods PEFT methods from various fields on the challenging PB\_T50\_RS variant of ScanObjectNN~\cite{uy2019revisiting}. As shown in the Tab.~\ref{tab:compare}, our methods are more effective than those proposed in NLP, 2D, and 3D areas. 

\textbf{Object Classification on Synthetic Dataset.} We conduct experiments on the ModelNet dataset~\cite{wu20153d}, which contains 12,311 clean 3D CAD models spanning 40 categories. Following the protocol in DAPT~\cite{zhou2024dynamic}, we split ModelNet40 into 9,843 training samples and 2,468 testing samples. Standard data augmentation techniques, including random scaling and random translation, were applied during training. As shown in Tab.~\ref{tab:sota}, without voting, PLT achieves accuracy rates of 93.8\% and 93.5\% on PointMAE and PointBERT, respectively—representing gains of 0.6\% and 0.8\% over full fine-tuning. When using voting, PLT continues to outperform full fine-tuning, notably improving PointBERT’s accuracy by an additional 1.0\%.

\textbf{Few-shot Learning.} We further evaluate PLT’s transfer learning capabilities in a few-shot setting on ModelNet40. Adopting the n-way, n-shot setup used in prior works~\cite{pang2022masked,zha2023instance,zhou2024dynamic}, we observe that PLT consistently outperforms fully fine-tuned models and state-of-the-art methods such as IDPT~\cite{zha2023instance} and DAPT~\cite{zhou2024dynamic} across most scenarios, demonstrating its efficacy in few-shot learning (see Tab.~\ref{tab:fewshot}).

\textbf{Comparison with Other PETL Methods.} As illustrated in Tab.~\ref{tab:origin_finetuning}, using VPT~\cite{jia2022visual} for fine-tuning in PointMAE~\cite{pang2022masked} results in a marked performance drop compared to full fine-tuning, with a decrease of 4.09\% in the PB\_T50\_RS setting. Similarly, while adapters~\cite{houlsby2019parameter} show modest improvements on the OBJ\_ONLY task, they exhibit significant drops in other scenarios. LST~\cite{sung2022lst} provides substantial gains on OBJ\_ONLY but underperforms in other settings. We further compare various PEFT methods across challenging PETL scenarios, including the PB\_T50\_RS variant of ScanObjectNN~\cite{uy2019revisiting}. As shown in Tab.~\ref{tab:compare}, our PLT method demonstrates superior effectiveness across NLP, 2D, and 3D applications.

\input{tab/fewshot}

\input{tab/compare}

\input{fig/s3dis}

\input{tab/origin_finetuning}

\input{tab/ablation}

\subsection{3D Dense Prediction Task}
For dense prediction tasks, including part segmentation and semantic segmentation, we adopt a prediction head similar to that of PointNext, allowing us to effectively utilize multi-scale information and enhance performance while maintaining a low number of trainable parameters.

We validate the effectiveness of PLT on the ShapeNetPart dataset~\cite{yi2016scalable}. As shown in Tab.~\ref{tab:segmentation}, PLT achieves results comparable to other methods in terms of instance-level mIoU (Inst. mIoU) while achieving notable improvements in class-level mIoU (Cls. mIoU). Specifically, PLT improves Cls. mIoU on PointBERT by 0.5\% over DAPT, demonstrating its effectiveness in capturing fine-grained details.

To evaluate PLT in the semantic segmentation task, we use the S3DIS dataset~\cite{armeni20163d}, with results presented in Tab.~\ref{tab:semantic_segmentation}. When using ACT as the baseline, PLT shows significant improvements of 7.2\%, 4.7\%, 4.8\%, and 1.9\% over IDPT, PointPEFT, DAPT, and PointGST, respectively. Likewise, with PointMAE as the baseline, PLT consistently outperforms other methods, further validating its effectiveness in dense prediction tasks. Additionally, as illustrated in Fig.~\ref{fig:s3dis}, the semantic segmentation results on the S3DIS dataset demonstrate that PLT achieves clearer segmentation boundaries and fewer misclassifications compared to other methods.


\input{tab/segmentation}

\input{tab/part}

\input{tab/fusion_way}

\input{tab/semantic_segmentation}

\input{tab/sidenet}

\subsection{Ablation Study}

\textbf{Analysis of Each Component.} We conduct experiments to evaluate the effectiveness of the proposed components in PLT. As shown in Tab.~\ref{tab:part}, the performance on PB T50 RS reaches 85.46\% when all modules are included. However, removing any single module results in a performance decrease, emphasizing the importance of each component.

\textbf{Hyperparameter Ablation for Hierarchical Ladder Network.} We further explore the hyperparameters of the Hierarchical Ladder Network (HLN), as displayed in Tab.~\ref{tab:ablation}. Results indicate that optimal performance is achieved with 3 layers, feature dimensions of [16, 32, 64, 128], and neighbor counts of [16, 8, 4].

\textbf{Fusion Strategy for Local and Global Information.} In Tab.~\ref{tab:fusion_way}, we assess various fusion methods and observe that the proposed Local-Global Fusion (LGF) with Softmax yields the best results. Notably, using only local information performs worse than using only global information, possibly because local features are learned from scratch, whereas global features leverage pre-trained prior knowledge.

\textbf{Comparison Between HLN and PLT.} Tab.~\ref{tab:sidenet} shows that training with HLN alone leads to a substantial performance drop of 4.72\% compared to full fine-tuning, likely due to the lack of pre-trained global priors. However, integrating HLN with the backbone network through PLT results in higher performance than full fine-tuning.

%\subsection{Ablation Study}

%\textbf{Analysis on each component.}We conduct experiments to prove the effectiveness of the proposed components of Our PLT. As shown in the Tab.~\ref{tab:part}, when all modules are used, the performance on PB T50 RS reaches 85.46\%. However, removing any of the proposed modules leads to performance degradation, further highlighting the necessity of each module.

%\textbf{Ablation on hyper-parameters.} We conduct an in-depth exploration of the hyperparameters of Hierarchical Ladder Network (HLN), as shown in the Tab.~\ref{tab:ablation}, and found that the performance is optimal when the number of layers is 3, with feature dimensions of [16, 32, 64, 128], and the number of neighbors set to [16, 8, 4].

%\textbf{Fusion way for or local and global information in point clouds.} As shown in the Tab.~\ref{tab:fusion_way}, we explored various fusion methods and found that the proposed LGF with Softmax achieved the best performance. Additionally, when using only local information, the performance was worse than using only global information. This may be due to the fact that local information is learned from scratch, while global information benefits from pre-trained prior knowledge.

%\textbf{Comparison between HLN and PLT.} As shown in the Tab.~\ref{tab:sidenet}, when training with only HLN, we observed a significant performance drop of 4.72\% compared to full fine-tuning, likely due to the absence of pre-trained global prior information. However, by using our proposed PLT to integrate the features of both HLN and the backbone network, we achieved superior performance than full fine-tuning.

% \subsection{Visualization}