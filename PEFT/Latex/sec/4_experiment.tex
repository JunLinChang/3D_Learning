\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Settings}
To ensure a fair comparison, we used the same experimental setup as the default fine-tuning method\cite{zha2023instance, zhou2024dynamic} for each baseline. During training, we freeze the weights of the pre-trained model and only fine-tune a small number of parameters for adding modules. All experiments were conducted on a single GeForce RTX 3090 GPU.

\input{tab/sota}

\subsection{3D Object Classification}
\textbf{Object Classification on Real-World Dataset.} Pre-trained Point cloud models are typically trained on the ShapeNet dataset\cite{chang2015shapenet}, which consists of clean, uniformly distributed point clouds. However, real-world point clouds often suffer from issues such as noise and missing point, leading to diverse and challenging distributions. To evaluate the performance in these more realistic conditions, we use the ScanObjectNN dataset\cite{uy2019revisiting}, which contains approximately 15k point cloud samples across 15 categories. These point clouds are captured in indoor scenes and often include background interference and occlusions from other objects. As shown in Tab.\ref{tab:sota} We conduct experiments on on three variants of the ScanObjectNN dataset\cite{uy2019revisiting} (OBJ\_BG, OBJ\_ONLY, and PB\_T50\_RS), using two baseline models, PointBERT\cite{yu2022point} and PointMAE\cite{pang2022masked}, to assess the effectiveness of our PLT. Notably, PLT achieved better accuracy than full fine-tuning across all settings while utilizing only 2.71\% of the parameters. Especially, PLT achieved increases of 4.14\%, 1.73\%, and 3.02\% in the three ScanObjectNN\cite{uy2019revisiting} variants on Point-BERT. Compared to the state-of-the-art model PointGST, our LST improved accuracy by 0.45\% on PointBERT\cite{yu2022point} and 0.17\% on PointMAE\cite{pang2022masked} under the most challenging setting, PB\_T50\_RS, in ScanObjectNN\cite{uy2019revisiting}.

\textbf{Object Classification on Synthetic Dataset.} We also conduct experiments on the ModelNet dataset\cite{wu20153d}, which includes 12,311 clean 3D CAD models across 40 categories. Following DAPT\cite{zhou2024dynamic}, we split the ModelNet40 dataset into 9,843 training samples and 2,468 testing samples. During training, we applied standard data augmentation techniques, including random scaling and random translation. As shown in Tab.\ref{tab:sota}, without voting, our PLT achieved accuracy rates of 93.8\% and 93.5\% on PointMAE and PointBERT, respectively, which were 0.6\% and 0.8\% higher than full fine-tuning.. With voting, PLT continues to outperform full fine-tuning, especially on PointBERT, with an accuracy increase of 1.0\%.

\textbf{Few-shot Learning.}We further conduct experiments on ModelNet40 to evaluate its transfer learning ability in few-shot setting. Following prior work\cite{pang2022masked,zha2023instance,zhou2024dynamic}, we adopt the n-way, n-shot setting. As shown in Tab.\ref{tab:fewshot}, PLT outperforms fully fine-tuned models and state-of-the-art models like IDPT\cite{zha2023instance} and DAPT\cite{zhou2024dynamic} in most cases, demonstrating its effectiveness in few-shot learning.

\textbf{Compared with Other PETL Methods.}We also compare PEFT methods from different fields on the most challenging PB\_T50\_RS variant of ScanObjectNN\cite{uy2019revisiting}. As shown in the Tab.\ref{tab:compare}, our methods are more effective than those proposed in NLP, 2D, and 3D areas.

\input{tab/fewshot}

\input{tab/compare}

\subsection{3D Dense Prediction Task}
For dense prediction tasks, such as part segmentation and semantic segmentation, we employ a prediction head similar to that of PointNext. This design allows us to leverage multi-scale information, enhancing performance while minimizing the number of trainable parameters.

We validate the effectiveness of PLT on ShapeNetPart dataset\cite{chang2015shapenet}. As shown in the Tab.\ref{tab:segmentation}, PLT achieves comparable results on Inst. mIoU, while significantly improving Cls. mIoU, particularly on PointBERT, where it outperforms DAPT by 0.5\%.

We evaluate the proposed PLT on the semantic segmentation task using the S3DIS dataset\cite{armeni20163d}, with results shown in Tab.\ref{tab:semantic_segmentation}. It is clear that our PLT significantly outperforms other methods. When using ACT as the baseline, PLT achieves improvements of 7.2\%, 4.7\%, 4.8\%, and 1.9\% over IDPT, PointPEF, DAPT, and PointGST, respectively. Similarly, when using PointMAE as the baseline, PLT consistently outperforms other methods, further validating its effectiveness in dense prediction tasks.

\input{tab/segmentation}

\input{tab/ablation}

\input{tab/semantic_segmentation}

\input{tab/part}

\input{tab/fusion_way}

\input{tab/sidenet}

\subsection{Ablation Study}

\textbf{Analysis on each component.}We conduct experiments to prove the effectiveness of the proposed components of Our PLT. As shown in the Tab.\ref{tab:part}, when all modules are used, the performance on PB T50 RS reaches 85.46\%. However, removing any of the proposed modules leads to performance degradation, further highlighting the necessity of each module.

\textbf{Ablation on hyper-parameters.} We conduct an in-depth exploration of the hyperparameters of Hierarchical Ladder Network (HLN), as shown in the Tab.\ref{tab:ablation}, and found that the performance is optimal when the number of layers is 3, with feature dimensions of [16, 32, 64, 128], and the number of neighbors set to [16, 8, 4].

\textbf{Fusion way for or local and global information in point clouds.} As shown in the Tab.\ref{tab:fusion_way}, we explored various fusion methods and found that the proposed LGF with Softmax achieved the best performance. Additionally, when using only local information, the performance was worse than using only global information. This may be due to the fact that local information is learned from scratch, while global information benefits from pre-trained prior knowledge.

\textbf{Comparison between HLN and PLT.} As shown in the Tab.\ref{tab:sidenet}, when training with only HLN, we observed a significant performance drop of 4.72\% compared to full fine-tuning, likely due to the absence of pre-trained global prior information. However, by using our proposed PLT to integrate the features of both HLN and the backbone network, we achieved superior performance than full fine-tuning.

% \subsection{Visualization}