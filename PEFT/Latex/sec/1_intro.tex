\section{Introduction}
\label{sec:intro}

With the growing accessibility of 3D scanning technology, 3D point cloud learning has emerged as a popular area of research with wide-ranging applications in fields such as autonomous driving, VR/AR, and robotics.In contrast to images, point clouds are unstructured, sparse, and permutation-invariant, presenting significant challenges for analysis and processing.Therefore, deep learning methods\cite{qi2017pointnet, li2018pointcnn,qian2022pointnext,qi2017pointnet++,wang2019dynamic,wu2024point,wu2022point,zhang2022patchformer,park2022fast,zhao2021point,guo2021pct,ma2022rethinking} for point cloud learning have been developed with specialized modules designed to directly process point clouds, resulting in significant performance improvements.

\input{fig/sota}

Recently, inspired by the success of pre-trained models in natural language processing\cite{devlin2018bert,brown2020language,2020ALBERT,raffel2020exploring,touvron2023llama,team2024gemma} and computer vision\cite{he2020momentum,chen2020improved,chen2021empirical,he2022masked,xie2022simmim,yeh2022decoupled}, a series of works\cite{pang2022masked,yu2022point,zhang2022point,afham2022crosspoint,dong2022autoencoders,liu2022masked,xie2020pointcontrast,qi2023contrast,wang2021unsupervised} have also emerged in the field of point cloud analysis.After pre-training, these methods employ a classic full fine-tuning strategy to adapt to downstream tasks, resulting in significant performance improvements and faster convergence compared to training from scratch.However, fully fine-tuning techniques may be suboptimal for point cloud processing for three reasons: 1) Updating all parameters of the pre-trained model can lead to overfitting and catastrophic forgetting, undermining the rich embedding knowledge acquired during the pre-training phase and resulting in poor performance. 2) Each point cloud analysis task and dataset requires separate copies of model parameters, which can pose storage challenges as demand increases. 3) To fully leverage the prior knowledge from the pre-training dataset, large models are often necessary. Full fine-tuning can incur significant computational costs, including increased GPU memory usage and extended training times, limiting accessibility for researchers with poor hardware resources.

To alleviate these problem, we will shift our research focus to Parameter Efficient Fine-tuning(PEFT)\cite{houlsby2019parameter,jie2023fact,karimi2021compacter,jia2022visual,lian2022scaling,sung2022lst,hu2021lora,zaken2022bitfit,li2021prefix,chen2022adaptformer,lester-etal-2021-power} in NLP and computer vision, which freezes most parameters of pretrained models, only a few selected parameters or some other ones inserted are trainable during tuning, allowing us to achieve performance comparable to or even better than full fine-tuning. For language and visual models, several pioneering works have emerged: 1) Adapter\cite{houlsby2019parameter,chen2022adaptformer} inserts lightweight networks after the Attention and Feed-Forward Network (FFN) layers in Transformers. 2) Prompt Tuning\cite{li2021prefix,lester-etal-2021-power,jia2022visual} adds a small number of learnable parameters to the token sequence. 3) Ladder Side Tuning\cite{sung2022lst} utilizes a small, independent network called Ladder, which takes intermediate activations as input and fine-tunes them through quick connections to the backbone network.These methods have achieved impressive results.

However, we empirically find that directly applying fine-tuning methods developed for language and vision models to the point cloud domain can lead to suboptimal performance. As illustrated in Tab.\ref{tab:origin_finetuning}, using VPT\cite{jia2022visual} for fine-tuning in PointMAE\cite{pang2022masked} resulted in a notable performance decline compared to full fine-tuning across various configurations of the ScanObjectNN\cite{uy2019revisiting}, particularly with a 4.09\% decrease in the PB\_T5\_RS setting. Similarly, while adapters\cite{houlsby2019parameter} showed slight improvements on the OBJ\_ONLY task, they also resulted in significant performance drops in other settings. Ladder Side Tuning (LST)\cite{sung2022lst} demonstrated substantial gains on OBJ\_ONLY, but performance in other settings remained inadequate. This raises a critical question: \textbf{how can we design an efficient and effective fine-tuning method tailored specifically for point clouds that achieves performance comparable to or even surpassing that of full fine-tuning?}

Therefore, in this paper, we propose a novel point cloud fine-tuning method based on Ladder Side Tuning, called Point Ladder Tuning (PLT). While the attention mechanism in the backbone network effectively captures global semantic information, it usually lacks local detail. To this end, we introduce a hierarchical Ladder Network designed to capture local information directly from the raw input. Additionally, we propose a local-global fusion module(LGF) that adaptively aggregates local information from the Ladder Network with global information activated in the backbone network, resulting in multi-scale representations that are crucial for enhancing network performance. To better adapt the pre-trained backbone network to downstream tasks, we also propose a simple and adaptive prompt generation method, which involves learnable scaling and translation of the fused features. Each prompt captures instance-specific multi-scale features and is easy to optimize, enabling the backbone network to adjust its instance-specific features more effectively.

Extensive experiments conducted across various point cloud datasets and settings have demonstrated the effectiveness of our method. As illustrated in the figure, our approach achieves an accuracy increase of 0.28\% compared to PointMAE\cite{pang2022masked} while utilizing only 2.71\% of the parameters in the PB\_T5\_S setting of the ScanObjectNN  dataset\cite{uy2019revisiting}, and a 3.02\% increase compared to PointBert with full fine-tuning.On the ShapeNetPart dataset\cite{wu20153d}, our method achieves an instance mIoU improvement of 0.1 on PointMAE\cite{pang2022masked} and 0.3 on PointBert\cite{yu2022point} utilizing less than 40\% of the parameters used by the current state-of-the-art method PointGST. Furthermore, on the S3DIS dataset\cite{armeni20163d} for point cloud semantic segmentation, our method improves mIoU by 0.7 over PointMAE and by 1.9 over ACT\cite{dong2022autoencoders} compared to PointGST\cite{liang2024parameter}, underscoring its superior performance.

Our main contributions can be summarized as follows:

\begin{itemize}
\item We present Point Ladder Tuning (PLT), a fine-tuning method for point clouds based on Ladder Side Tuning (LST). PLT utilizes a hierarchical Ladder Network to extract local information and a local-global fusion module to integrate this with global features, yielding rich multi-scale representations.
\item To further enhance performance, we present a simple and effective prompt generation module that linearly maps the output of the local-global fusion to inject multi-scale information into the backbone network.
\item Extensive experiments have demonstrated that our PLT achieves performance that is comparable to, or even surpasses, full fine-tuning across various tasks and datasets while utilizing very few parameters.
\end{itemize}

\input{tab/origin_finetuning}
