\section{Introduction}
\label{sec:intro}

\blk{With the growing accessibility of 3D scanning technology, 3D point cloud learning has emerged as a popular area of research with wide-ranging applications in fields such as autonomous driving, VR/AR, and robotics.In contrast to images, point clouds are unstructured, sparse, and permutation-invariant, presenting significant challenges for analysis and processing.Therefore, deep learning methods~\cite{qi2017pointnet, li2018pointcnn,qian2022pointnext,qi2017pointnet++,wang2019dynamic,wu2024point,wu2022point,zhang2022patchformer,park2022fast,zhao2021point,guo2021pct,ma2022rethinking} for point cloud learning have been developed with specialized modules designed to directly process point clouds, resulting in significant performance improvements.}
\input{fig/sota}
\input{tab/origin_finetuning}


	The growing accessibility of 3D scanning technology has elevated 3D point cloud learning to an emerging research area with diverse applications across computer vision and graphics fields, including autonomous driving (add citation), virtual and augmented reality (VR/AR) (add citation), and robotics (add citation).  
	Unlike images, point clouds are inherently unstructured, sparse, and permutation-invariant, which poses unique challenges for effective analysis and processing. 
	Consequently, deep learning-based methods~\cite{qi2017pointnet, li2018pointcnn,qian2022pointnext,qi2017pointnet++,wang2019dynamic,wu2024point,wu2022point,zhang2022patchformer,park2022fast,zhao2021point,guo2021pct,ma2022rethinking} specifically tailored for point cloud learning have been developed, incorporating specialized modules to directly handle point cloud data and achieving substantial improvements in performance.

\blk{Recently, inspired by the success of pre-trained models in natural language processing~\cite{devlin2018bert,brown2020language,2020ALBERT,raffel2020exploring,touvron2023llama,team2024gemma} and computer vision~\cite{he2020momentum,chen2020improved,chen2021empirical,he2022masked,xie2022simmim,yeh2022decoupled}, a series of works~\cite{pang2022masked,yu2022point,zhang2022point,afham2022crosspoint,dong2022autoencoders,liu2022masked,xie2020pointcontrast,qi2023contrast,wang2021unsupervised} have also emerged in the field of point cloud analysis.After pre-training, these methods employ a classic full fine-tuning strategy to adapt to downstream tasks, resulting in significant performance improvements and faster convergence compared to training from scratch.However, fully fine-tuning techniques may be suboptimal for point cloud processing for three reasons: 1) Updating all parameters of the pre-trained model can lead to overfitting and catastrophic forgetting, undermining the rich embedding knowledge acquired during the pre-training phase and resulting in poor performance. 2) Each point cloud analysis task and dataset requires separate copies of model parameters, which can pose storage challenges as demand increases. 3) To fully leverage the prior knowledge from the pre-training dataset, large models are often necessary. Full fine-tuning can incur significant computational costs, including increased GPU memory usage and extended training times, limiting accessibility for researchers with poor hardware resources.}

Inspired by the success of pre-trained models in natural language processing~\cite{devlin2018bert,brown2020language,2020ALBERT,raffel2020exploring,touvron2023llama,team2024gemma} and computer vision~\cite{he2020momentum,chen2020improved,chen2021empirical,he2022masked,xie2022simmim,yeh2022decoupled}, recent research has extended this approach to point cloud analysis~\cite{pang2022masked,yu2022point,zhang2022point,afham2022crosspoint,dong2022autoencoders,liu2022masked,xie2020pointcontrast,qi2023contrast,wang2021unsupervised}. Following pre-training, these methods typically use a full fine-tuning strategy to adapt the models to downstream tasks, yielding significant performance gains and faster convergence compared to training from scratch. However, full fine-tuning may be suboptimal for point cloud analysis for several reasons: (1) Updating all parameters of the pre-trained model can lead to overfitting and catastrophic forgetting, undermining the rich embeddings learned during pre-training and resulting in degraded performance. (2) Each point cloud analysis task and dataset requires a separate model parameter set, creating storage challenges as demand scales. (3) To leverage the prior knowledge from pre-training fully, large models are often needed. Full fine-tuning incurs substantial computational costs, including increased GPU memory usage and extended training times, which can limit accessibility for researchers with limited hardware resources.



\blk{To alleviate these problem, we will shift our research focus to Parameter Efficient Fine-tuning(PEFT)~\cite{houlsby2019parameter,jie2023fact,karimi2021compacter,jia2022visual,lian2022scaling,sung2022lst,hu2021lora,zaken2022bitfit,li2021prefix,chen2022adaptformer,lester-etal-2021-power} in NLP and computer vision, which freezes most parameters of pretrained models, only a few selected parameters or some other ones inserted are trainable during tuning, allowing us to achieve performance comparable to or even better than full fine-tuning. For language and visual models, several pioneering works have emerged: 1) Adapter~\cite{houlsby2019parameter,chen2022adaptformer} inserts lightweight networks after the Attention and Feed-Forward Network (FFN) layers in Transformers. 2) Prompt Tuning~\cite{li2021prefix,lester-etal-2021-power,jia2022visual} adds a small number of learnable parameters to the token sequence. 3) Ladder Side Tuning~\cite{sung2022lst} utilizes a small, independent network called Ladder, which takes intermediate activations as input and fine-tunes them through quick connections to the backbone network.These methods have achieved impressive results.}

To address these challenges, our research will focus on Parameter-Efficient Fine-Tuning (PEFT)~\cite{houlsby2019parameter,jie2023fact,karimi2021compacter,jia2022visual,lian2022scaling,sung2022lst,hu2021lora,zaken2022bitfit,li2021prefix,chen2022adaptformer,lester-etal-2021-power}, an approach popular in NLP and computer vision that freezes most parameters of pre-trained models, making only a few selected parameters—or newly introduced parameters—trainable during fine-tuning. This approach allows performance comparable to, or even exceeding, full fine-tuning while reducing parameter updates. Several pioneering PEFT methods have been developed for language and visual models: (1) Adapters~\cite{houlsby2019parameter,chen2022adaptformer}, which insert lightweight networks after Attention and Feed-Forward Network (FFN) layers in Transformers; (2) Prompt Tuning~\cite{li2021prefix,lester-etal-2021-power,jia2022visual}, which adds a small number of learnable parameters to the token sequence; and (3) Ladder Side Tuning~\cite{sung2022lst}, which employs a compact, independent network called Ladder, designed to take intermediate activations and refine them through rapid connections to the backbone network. These PEFT techniques have demonstrated notable performance improvements.

\blk{However, we empirically find that directly applying fine-tuning methods developed for language and vision models to the point cloud domain can lead to suboptimal performance. As illustrated in Tab.\ref{tab:origin_finetuning}, using VPT~\cite{jia2022visual} for fine-tuning in PointMAE~\cite{pang2022masked} resulted in a notable performance decline compared to full fine-tuning across various configurations of the ScanObjectNN~\cite{uy2019revisiting}, particularly with a 4.09\% decrease in the PB\_T5\_RS setting. Similarly, while adapters~\cite{houlsby2019parameter} showed slight improvements on the OBJ\_ONLY task, they also resulted in significant performance drops in other settings. Ladder Side Tuning (LST)~\cite{sung2022lst} demonstrated substantial gains on OBJ\_ONLY, but performance in other settings remained inadequate. This raises a critical question: \textbf{how can we design an efficient and effective fine-tuning method tailored specifically for point clouds that achieves performance comparable to or even surpassing that of full fine-tuning?}}

However, our empirical findings in Sec.\ref{sec:classification} indicate that directly applying fine-tuning methods developed for language and vision models to the point cloud domain often results in suboptimal performance. These findings highlight a key problem: \textbf{how can we develop a fine-tuning approach specifically designed for point clouds that not only matches but potentially exceeds the performance of full fine-tuning while being both efficient and effective?} Addressing this challenge is crucial for advancing parameter-efficient techniques in point cloud analysis, given the unique characteristics and demands of 3D data.

\blk{Therefore, in this paper, we propose a novel point cloud fine-tuning method based on Ladder Side Tuning, called Point Ladder Tuning (PLT). While the attention mechanism in the backbone network effectively captures global semantic information, it usually lacks local detail. To this end, we introduce a hierarchical Ladder Network designed to capture local information directly from the raw input. Additionally, we propose a local-global fusion module(LGF) that adaptively aggregates local information from the Ladder Network with global information activated in the backbone network, resulting in multi-scale representations that are crucial for enhancing network performance. To better adapt the pre-trained backbone network to downstream tasks, we also propose a simple and adaptive prompt generation method, which involves learnable scaling and translation of the fused features. Each prompt captures instance-specific multi-scale features and is easy to optimize, enabling the backbone network to adjust its instance-specific features more effectively.}

\blk{Extensive experiments conducted across various point cloud datasets and settings have demonstrated the effectiveness of our method. As illustrated in the figure, our approach achieves an accuracy increase of 0.28\% compared to PointMAE~\cite{pang2022masked} while utilizing only 2.71\% of the parameters in the PB\_T5\_S setting of the ScanObjectNN  dataset~\cite{uy2019revisiting}, and a 3.02\% increase compared to PointBert with full fine-tuning.On the ShapeNetPart dataset~\cite{wu20153d}, our method achieves an instance mIoU improvement of 0.1 on PointMAE~\cite{pang2022masked} and 0.3 on PointBert~\cite{yu2022point} utilizing less than 40\% of the parameters used by the current state-of-the-art method PointGST. Furthermore, on the S3DIS dataset~\cite{armeni20163d} for point cloud semantic segmentation, our method improves mIoU by 0.7 over PointMAE and by 1.9 over ACT~\cite{dong2022autoencoders} compared to PointGST~\cite{liang2024parameter}, underscoring its superior performance.}

In this paper, we introduce a novel point cloud fine-tuning approach based on Ladder Side Tuning, termed Point Ladder Tuning (PLT). While the attention mechanism in the backbone network effectively captures global semantic information, it often lacks finer local detail. To address this, we design a hierarchical Ladder Network specifically to extract local information directly from the raw input. Additionally, we propose a Local-Global Fusion (LGF) module, which adaptively combines the local information from the Ladder Network with global features activated within the backbone network, producing multi-scale representations essential for improving network performance. 
To further optimize the pre-trained backbone network for downstream tasks, we introduce an adaptive prompt generation technique. This method learns to scale and translate the fused features, creating instance-specific multi-scale prompts that are easily optimized and enable the backbone to refine its instance-specific features more effectively.

% \lrp{Extensive experiments across diverse point cloud datasets and configurations validate the effectiveness of our method. As illustrated in the results, our approach achieves a 0.28\% accuracy increase compared to PointMAE~\cite{pang2022masked} while using only 2.71\% of the parameters in the PB\_T5\_S setting of the ScanObjectNN dataset~\cite{uy2019revisiting}, and a 3.02\% gain over PointBert with full fine-tuning. On the ShapeNetPart dataset~\cite{wu20153d}, our approach shows an instance mIoU improvement of 0.1 over PointMAE~\cite{pang2022masked} and 0.3 over PointBert~\cite{yu2022point} using less than 40\% of the parameters compared to the current state-of-the-art method, PointGST. Moreover, on the S3DIS dataset~\cite{armeni20163d} for point cloud semantic segmentation, PLT improves mIoU by 0.7 over PointMAE and by 1.9 over ACT~\cite{dong2022autoencoders} compared to PointGST~\cite{liang2024parameter}, underscoring its superior performance and parameter efficiency.} (move to experiments section?)
% \input{tab/origin_finetuning}

Our main contributions can be summarized as follows:

\begin{itemize}
\item We propose Point Ladder Tuning (PLT), a fine-tuning method for point cloud data built upon Ladder Side Tuning (LST). PLT employs a hierarchical Ladder Network to extract local information, and a Local-Global Fusion (LGF) module to integrate this local information with global features, producing rich multi-scale representations.
\item To further enhance network performance, we introduce a straightforward yet effective prompt generation module that linearly maps the output of the LGF module, injecting multi-scale information directly into the backbone network.
\item Extensive experiments demonstrate that PLT achieves performance comparable to, and often surpassing, full fine-tuning across a range of tasks and datasets, while requiring significantly fewer parameters.
\end{itemize}


